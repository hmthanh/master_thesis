# A Comprehensive Review of Data-Driven Co-Speech Gesture Generation

### Abstract

- Gestures that accompany speech are important for natural and efficient embodied human communication.
- Automatic generation of co-speech gestures is a long-standing problem in computer animation and is considered enabling technology for creating believable characters in film, games, and virtual social spaces, as well as for interaction with social robots.
- The problem is challenging due to the idiosyncratic and non-periodic nature of human co-speech gesture motion and the great diversity of communicative functions that gestures encompass.
- There is a surge of interest in gesture generation due to the emergence of larger datasets of human gesture motion and strides in deep-learning-based generative models that benefit from the growing availability of data.
- The article summarizes co-speech gesture generation research, focusing on deep generative models.
- The theory describing human gesticulation and how it complements speech is articulated, and rule-based and classical statistical gesture synthesis is briefly discussed before delving into deep learning approaches.
- Input modalities are employed as an organizing principle, examining systems that generate gestures from audio, text, and non-linguistic input.
- The evolution of related training datasets in terms of size, diversity, motion quality, and collection method is chronicled.
- Key research challenges in gesture generation are identified, including data availability and quality, producing human-like motion, grounding the gesture in the co-occurring speech in interaction with other speakers and the environment, performing gesture evaluation, and integration of gesture synthesis into applications.
- Recent approaches to tackling the key challenges, as well as their limitations, are highlighted, and areas of future development are pointed toward.

## 1. Introduction

- Gesture (cử chỉ) là một phần cần thiết trong giao tiếp nhân thể và có ít nhất ba chức năng chính: giúp các tác nhân nhân tạo trông sống động hơn và thu hút hơn; truyền tải thông tin chức năng; truyền tải thông tin xã hội.
- Sự tồn tại của gesture có thể cải thiện hiệu suất giao tiếp trong các cuộc đàm phán và giao tiếp trực tiếp.
- Các dạng gesture đa dạng và không đều nhau, điều này làm cho việc tự động tạo ra chúng trở nên khó khăn.
- Sự phát triển của deep learning và sự xuất hiện của các tập dữ liệu lớn hơn đã thúc đẩy sự quan tâm đến việc tạo ra cử chỉ đồng hành với lời nói bằng deep learning.
- Các phương pháp tạo ra gesture bao gồm: tạo ra cử chỉ dựa trên quy tắc, các phương pháp thống kê cổ điển và các phương pháp sử dụng deep learning.
- Deep learning cung cấp nhiều tiếp cận khác nhau để tạo ra gesture, trong đó đầu vào có thể là âm thanh, văn bản và các dạng đầu vào phi ngôn ngữ khác.
- Tập dữ liệu dùng để huấn luyện các mô hình gesture cũng phát triển theo thời gian về kích thước, đa dạng, chất lượng chuyển động và phương pháp thu thập dữ liệu.
- Các thách thức chính trong việc tạo ra cử chỉ đồng hành với lời nói bao gồm:
  - Sự sẵn có và chất lượng của dữ liệu; tạo ra chuyển động giống như con người;
  - Đưa cử chỉ vào ngữ cảnh của đối thoại và môi trường
  - Đánh giá cử chỉ
  - Tích hợp phát hiện cử chỉ vào các ứng dụng.
- Các phương pháp mới gần đây đã đưa ra giải pháp cho các thách thức trên, tuy nhiên vẫn còn hạn chế và cần phát triển thêm.

- The paper focuses on co-verbal gesture, which accompanies speech.
- Gesture plays at least three main functions:
  - Making artificial agents and robots look more alive and engaging
  - communicating functional information
  - And communicating social information.
- Functional information can include pointing, deictic gesture, emblems, and imagistic metaphoric and iconic gestures.
- Social information communicated through gesture can include personality, emotion, and subtext.
- Gestures communicate in a different way than spoken language and can support a range of applications for virtual agents and robots.
- It has been established that gestures do indeed communicate, and there are three main findings for when gestures communicate according to Hostetter’s meta-analysis: gestures depicting motor actions are more communicative than those depicting abstract topics, gestures that are not completely redundant have a larger impact on communication, and children benefit more from gesture than adults.

Overall, our survey makes the following contributions to the field:
A detailed discussion on the theory and motivation for co-verbal gesture synthesis.

- A discussion on rule-based and statistical techniques, illustrating how these approaches can complement the strengths and weak-nesses of recent deep-learning approaches.
- An emphasis on deep-learning-based generation systems using input modality as an organizing principle for the research.
- A discussion on the most commonly used speech-to-gesture datasets, collected via motion capture or pose estimation.
- Identifying and detailing a set of key challenges for co-verbal gesture synthesis and potential research directions.

## 2. Human gesticulation

- Cử chỉ có thể có nhiều hình thái khác nhau và tuân thủ các quy tắc hình thái học khác nhau.

  - Phân loại cử chỉ của Kendon và "Kendon's Continuum" của McNeill phân loại các cử chỉ dựa trên độ từ vựng.
  - Đầu cực độ từ vựng của phổ (ngôn ngữ ký hiệu) có cấu trúc cú pháp chính thức giống như ngôn ngữ nói, trong khi các cử chỉ giao tiếp thường không có cấu trúc và có thể đặc trưng riêng.

- McNeill đề xuất một quan điểm kích thước về cử chỉ, bao gồm kích thước biểu tượng, ẩn dụ, chỉ trỏ và nhịp đập.

  - Một cử chỉ có thể đặt trọng tâm trên nhiều kích thước và có thể truyền đạt tính cách qua tự sự điều chỉnh, được gọi là "adaptors".
  - Một cử chỉ biểu tượng có thể hiển thị kích thước của một chiếc hộp đang được thảo luận bằng cách vẽ nó trên không gian, trong khi một cử chỉ ẩn dụ có thể chỉ ra một khái niệm trừu tượng bằng cách tạo hình dáng ô dù.

- Thách thức trong tổng hợp cử chỉ cho tương tác giữa con người và máy tính.

  - Giả thuyết điểm tăng trưởng cho rằng cử chỉ và ngôn ngữ xuất phát từ cùng một ý đồ giao tiếp.
  - Nhiều phương pháp tổng hợp cử chỉ và phương pháp học sâu không mô hình hóa ý đồ giao tiếp, dẫn đến sự trùng lặp và giới hạn của cử chỉ.

- Tính đặc thù của cử chỉ và sự khác biệt giữa các cá nhân.
  - Các cá nhân khác nhau có thể cử chỉ bằng cách khác nhau, và cùng một người có thể tạo ra các cử chỉ khác nhau cho cùng một văn bản.
  - Cử chỉ được đồng bộ hóa theo thời gian với lời nói của họ, thường xuất hiện trước lời nói đồng cảm khoảng 90% thời gian.
  - Người ta có thể không quá nhạy cảm với sai sót thời gian trong phạm vi +/- 0.6 giây.

## 3. Approaches for gesture synthesis

- Tổng hợp các cử chỉ đồng nói là cần thiết để tạo ra các nhân vật ảo tương tác và đáng tin cậy trong tương tác giữa con người và máy tính (HCI), đồ họa và robot xã hội.

  - Vấn đề chính trong tổng hợp cử chỉ gồm vấn đề xác định cử chỉ và vấn đề tạo động cử chỉ.
  - Có nhiều phương pháp được sử dụng để xác định cử chỉ, bao gồm ngữ 音, văn bản và ý đồ giao tiếp, trong đó các mô hình dựa trên quy tắc, thống kê và học máy đã được sử dụng để xác định cử chỉ phù hợp.
  - Có hai loại chính của mô hình sinh cử chỉ:
    - Mô hình dựa trên quy tắc
    - Và mô hình dữ liệu. Trong mô hình dữ liệu, có hai loại con là
      - Mô hình thống kê
      - Mô hình học máy.

- Các phương pháp mô hình sinh cử chỉ.

  - Các hệ thống dựa trên quy tắc sử dụng các thuật toán heuristics được thiết kế cẩn thận để chọn cử chỉ phù hợp với đầu vào âm thanh được cung cấp.
  - Các hệ thống dữ liệu sử dụng các phương pháp thống kê hoặc mô hình phân bố để xác định cử chỉ phù hợp với đầu vào âm thanh.
  - Các mô hình học máy tối ưu hóa các thông số của một hàm phi tuyến phức tạp để ánh xạ đầu vào âm thanh vào cử chỉ phù hợp.
  - Các phương pháp học sâu có xu hướng tổng hợp chuyển động trên cơ sở từng khung hình.
  - Các phương pháp dựa trên quy tắc và thống kê thường dự đoán một nhãn cử chỉ được sử dụng để chỉ mục các đoạn chuyển động cử chỉ được thực hiện bằng tay hoặc được ghi sẵn. Trong khi đó, các phương pháp học sâu có xu hướng tổng hợp chuyển động trên cơ sở từng khung hình.

Hình 4 minh họa sự phát triển này và sự đánh đổi giữa tính tự nhiên và hiệu quả trong truyền đạt thông tin của các phương pháp khác nhau.

![diagram.png](diagram.png)

Các phương pháp sử dụng thống kê đã cải thiện tính tự nhiên của cử chỉ, nhưng đánh đổi với hiệu quả truyền đạt thông tin.
Ngược lại, các phương pháp dựa trên học sâu có thể tạo ra các cử chỉ liên tục và khá tự nhiên, nhưng hiệu quả truyền đạt thông tin lại thấp hơn.

Nghiên cứu gần đây đã đề xuất các hệ thống kết hợp các phương pháp khác nhau để tạo ra các cử chỉ tự nhiên và có ý nghĩa ngữ nghĩa. Đoạn văn cũng đề cập đến các phương pháp khác nhau, bao gồm các hệ thống dựa trên luật, thống kê và học sâu, và chỉ ra các nghiên cứu tiêu biểu trong từng lớp phương pháp này.

## 4. Rule-base approaches

Đoạn văn trình bày về hai hệ thống đầu tiên để tự động tạo ra cử chỉ tay, di chuyển khuôn mặt và mô phỏng âm điệu phù hợp với bối cảnh giữa các nhân vật giống như con người. Hệ thống đầu tiên được giới thiệu là Animated Conversation [CPB∗ 94], nó được phát triển dựa trên quan hệ ẩn giữa lời nói và cử chỉ.
Nó giới hạn trong việc tạo ra đối thoại giữa hai nhân vật trong một lĩnh vực nhất định. Ymir [Thó96] là hệ thống thứ hai được giới thiệu để cải tiến hệ thống Animated Conversation, cho phép đầu vào đa phương tiện từ người dùng bao gồm giọng nói, ánh mắt, cử chỉ và ngữ điệu. Tuy nhiên, hệ thống này chỉ có thể tạo ra đầu ra đa phương tiện hạn chế trong thời gian thực. Cuối cùng, hệ thống đối thoại hình thái Cassell et al. [CBC∗ 00] đã cải tiến hai hệ thống trước bằng cách tích hợp tính tương tác đa phương tiện trong thời gian thực của Ymir với khả năng mô phỏng đa phương tiện phong phú hơn của Animated Conversation.
Kết quả là một khung nhân vật đối thoại được thể hiện một cách linh hoạt và mạnh mẽ trong các cuộc trò chuyện, mặc dù vẫn giới hạn trong việc tạo ra đối thoại dựa trên cơ sở tri thức tĩnh.

- BEAT (Behaviour Expression Animation Toolkit) là một trong những công cụ tạo ra các cử chỉ không ngôn ngữ, được giới thiệu bởi Cassell và đồng nghiệp.
- BEAT sử dụng thông tin ngữ cảnh tiềm ẩn trong văn bản để chọn các chuyển động tay, cánh tay và khuôn mặt đã được ghi trước.
- Kopp và đồng nghiệp đã đề xuất một phương pháp dựa trên mô hình để tạo ra các lời nói đa phương tiện phức tạp từ các đặc tả XML về hình dạng của chúng.
- Greta, được phát triển bởi Pelechaud và đồng nghiệp, là một đại diện ảo 3D mà các cử chỉ khuôn mặt của nó đã truyền tải trạng thái cảm xúc của đại diện.
- Hướng nhìn, cử chỉ đầu và biểu cảm khuôn mặt là những hành vi phi ngôn ngữ quan trọng để truyền tải ý định và trạng thái cảm xúc của người nói.

- Những hệ thống mới trong dựa trên quy tắc thường đòi hỏi phải phát triển một ngôn ngữ đặc thù (domain-specific language - DSL), thường được xây dựng dựa trên XML.
- Có nhiều DSL khác nhau được sử dụng trong các hệ thống khác nhau.
- Một nhóm các nhà nghiên cứu đã phát triển một ngôn ngữ thống nhất cho việc tạo hành vi đa dạng cho các đại diện ảo (virtual agents) có tên là Behavior Markup Language (BML).
- BML được thiết kế trong bối cảnh của một framework toàn diện với các giai đoạn lập kế hoạch ý định (intent planning), lập kế hoạch hành vi (behavior planning) và thực hiện hành vi (behavior realization).
- BML trở thành định dạng tiêu chuẩn cho các hệ thống dựa trên quy tắc, được sử dụng trong các framework mã nguồn mở như SmartBody và các đại diện nhân tạo khác như các robot humanoid.

- Advances in rule-based systems

  - Facial expression and behavior generation by Marsella et al. [MXL∗ 13]
  - Metaphorical gesture generation by Ravenet et al. [RPCM18]
  - BML as a standard domain-specific language

- Limitations of rule-based systems

  - Reliance on pre-recorded motion or carefully engineered systems
  - Limited gesture distribution and flexibility in producing diverse gestures

- Data-driven methods for gesture generation

  - Statistical methods
  - Deep learning-based methods.

## 5. Data-driven approaches

### 5.1 Speech-Gesture Datasets

- Các phương pháp dựa trên dữ liệu phụ thuộc vào dữ liệu mà chúng được huấn luyện, và số lượng và kích thước của các bộ dữ liệu phù hợp cho việc học máy trên dữ liệu cử chỉ của con người đã tăng đáng kể theo thời gian.
- Có hai phương pháp chính để thu thập dữ liệu chuyển động cho việc tổng hợp cử chỉ: thu thập chuyển động quang học và ước tính tư thế từ video một mắt. Các bộ dữ liệu thu thập chuyển động quang học nhỏ hơn nhưng có chất lượng chuyển động 3D cao và có thể thu được cử chỉ của ngón tay, trong khi các bộ dữ liệu từ ước tính tư thế có thể lớn hơn và bao gồm nhiều cử chỉ đa dạng hơn, nhưng chất lượng chuyển động thấp và giới hạn trong không gian 2D.
- Lượng dữ liệu cần thiết phụ thuộc vào ứng dụng, nhưng chỉ cầ  n hai phút dữ liệu để điều chỉnh mô hình tạo cử chỉ cho một phong cách khác. Công trình nghiên cứu gần đây cũng đã chứng minh khả năng học nhúng các phong cách cử chỉ khác nhau, có thể được sử dụng cho việc thích nghi không cần dữ liệu huấn luyện với phong cách của một người nói mục tiêu chưa được huấn luyện. Các kỹ thuật tăng cường dữ liệu cử chỉ, chẳng hạn như đối xứng, cũng đã được nghiên cứu.

### 5.2. Statistical and early machine learning approaches

Đoạn văn mô tả về việc sử dụng các hệ thống thống kê để mô hình quan hệ giữa ngôn ngữ và cử chỉ. Hệ thống này giúp giảm bớt giả định về mối liên hệ giữa ngôn ngữ và cử chỉ, thay vào đó tính toán xác suất điều kiện cho dữ liệu cử chỉ hoặc phân bố xác suất trước cho dữ liệu cử chỉ.

Kipp đề xuất hệ thống thống kê sớm nhất để mô hình hóa cử chỉ của một cá nhân bằng cách phân tích một tập dữ liệu có chú thích về lời nói và tạo ra một hồ sơ cử chỉ. Nghiên cứu của Neff mở rộng phương pháp này bằng cách học các hồ sơ cử chỉ của nhân vật và thêm một từ điển hoạt hình cụ thể cho nhân vật đó.

Bergman và Kopp đề xuất một phương pháp thống kê khác để mô hình hóa sự biến đổi của ngôn ngữ mô tả đối tượng thành các cử chỉ biểu tượng tương tự như các đối tượng đó. Các tác giả sử dụng mạng Bayes để xác định phân bố xác suất cho các thuộc tính cử chỉ và kết hợp với kiến thức ảnh hưởng đến việc tạo hình cử chỉ. Kết quả là các đặc trưng chi tiết của cử chỉ như hình dạng tay, vị trí cổ tay, hướng của lòng bàn tay, hướng ngón tay, quỹ đạo và hướng chuyển động.

Cả ba phương pháp này đều sử dụng các hệ thống thống kê để mô hình hóa và tạo ra các cử chỉ phù hợp với ngôn ngữ, tuy nhiên mỗi phương pháp lại có những điểm khác nhau và được áp dụng trong các bối cảnh khác nhau.

Bergman và Kopp đề xuất một khung việc kết hợp kỹ thuật dựa trên dữ liệu và dựa trên mô hình để mô hình hóa các cử chỉ biểu tượng bằng cách sử dụng mạng quyết định Bayesian [BK09a]. Levine và đồng nghiệp đề xuất một mô hình Markov ẩn (HMM) để lựa chọn đoạn video chuyển động phù hợp nhất từ cơ sở dữ liệu chuyển động, sử dụng các đặc trưng dựa trên prosody được trích xuất từ tiếng nói gốc. Chiu và đồng nghiệp giữ nguyên việc sử dụng các đặc trưng prosodic để học mô hình xác suất cho việc tạo ra các cử chỉ. Yang và đồng nghiệp đề xuất một hệ thống dựa trên mô hình thống kê dựa trên đồ thị chuyển động để tạo ra các cử chỉ và các chuyển động khác cho cuộc trò chuyện hai người.

Các mô hình thống kê cung cấp tính linh hoạt hơn so với các hệ thống dựa trên quy tắc và bắt được sự không xác định trong các cử chỉ trong cuộc trò chuyện. Mặc dù các nguyên tắc thống kê được sử dụng phổ biến hơn, hiện nay đang có xu hướng chuyển sang các mô hình dựa trên học sâu có khả năng đại diện tốt hơn và không đưa ra quá nhiều giả định về thống kê của dữ liệu gốc.

### 5.3. Deep learning approaches

Deep learning-based generative models recently gained interest because of their ability to synthesize data from abstract representations of training datasets. They are increasingly prominent in character animation applications, including character control in games,and facial or gesture animation conditioned on speech and text invirtual agents. Such models typically make few assumptions about the underlying data distribution (except useful inductive biases),and learn their parameters to fit the data through gradient-based optimization of an objective function.

The use of deep-learning approaches has moved the field for-ward substantially in terms of perceived naturalness, but arguably represents a step backwards in terms of communicative efficacy with respect to previous methods, as illustrated in Figure 4. In-stead, the main targets of systems based on deep learning have been human-likeness and appropriateness for speech audio and semantic content. The former is the degree to which the gener-ated gesture motion visually resembles believable human behavior, while the later is how suitable it is for a given speech audio, text input, or other contextual information. Early deep-learning systems ignored semantics, instead focusing on improving human-likeness [AHKB20,KHH∗ 19,FNM20]. Later approaches have triedto incorporate semantics in order to generate meaningful gestures. The first attempts could generate only a handful of such gestures[KJvW∗ 20, YCL∗ 20, ALIM20]. Although more recent work sug-gest that progress can [KNN∗ 22] and has been made [AGL∗ 22],appropriateness remains a challenge. This can be seen from the GE-NEA Challenge (where GENEA stands for Generation and Evalu-ation of Non-verbal Behavior for Embodied Agents), which is a re-curring large-scale comparison of gesture synthesis systems, whose most recent iteration [YWK∗ 22] found that the human-likeness of motion can now reach the level of human motion capture, while appropriateness is still barely above chance.

The proliferation of deep learning in conversational gesture gen-eration has led to a large number of approaches that can be groupedbased on the input modalities, i.e. audio, text, audio and text, or au-dio with other non-communicative modalities, and control parame-ters. We employ this taxonomy to organize our exposition and givea summary of the models and their respective categories in Table 2. We only include approaches that produce hand gestures and were published before the submission deadline for our review. In sec-tions 5.3.1, 5.3.2, and 5.3.3 we discuss generation approaches thatuse audio-only, text-only, and a combination of audio and text in-put, respectively. Section 5.3.4 focuses on approaches that use non-linguistic input, i.e. input other than speech audio or text. Finally,Section 5.3.5 explores approaches that employ control input. The approaches within each modality section are presented in chrono-logical order to reflect the evolution of the field.

- Các mô hình sinh dựa trên học sâu gần đây thu hút sự quan tâm vì khả năng tổng hợp dữ liệu từ biểu diễn trừu tượng của tập dữ liệu huấn luyện. Chúng ngày càng nổi bật trong các ứng dụng hoạt hình nhân vật, bao gồm kiểm soát nhân vật trong trò chơi, hoạt hình khuôn mặt hoặc cử chỉ dựa trên âm thanh và văn bản trong đại lý ảo.

- Việc sử dụng các phương pháp học sâu đã thúc đẩy ngành phát triển đáng kể về tính tự nhiên nhưng có thể bước lùi về hiệu quả giao tiếp so với các phương pháp trước đó. Các hệ thống dựa trên học sâu chủ yếu nhắm đến độ giống người và phù hợp với âm thanh và nội dung ngữ nghĩa.

- Sự phổ biến của học sâu trong tạo ra cử chỉ đàm thoại đã dẫn đến nhiều phương pháp có thể được phân nhóm dựa trên các đầu vào như âm thanh, văn bản, âm thanh và văn bản, hoặc âm thanh với các đầu vào không giao tiếp khác. Bài báo này sắp xếp các phương pháp theo nhóm và tổng kết các mô hình và loại tương ứng trong Bảng 2.

#### 5.3.1. Audio input

Hasegawa et al. [HKS∗ 18] proposed an autoregressive approachto generate gesture from audio utterances using a bi-directionalLSTM [HS97]. The bi-directional LSTM learned audio-gesture re-lationships with both backward and forward consistencies over along period of time. The model was trained with a then novel audio-gesture dataset, collected using a headset and marker-based motioncapture [TKS∗ 17]. The model predicted a full skeletal human posefrom the utterance features input at every LSTM timestep. Tem-poral filtering was then used to smooth out discontinuities in thegenerated pose sequences.Kucherenko et al. [KHH∗ 19] extended the work of Hasegawa etal. [HKS∗ 18], removing the need for temporal smoothing throughrepresentation learning of an autoencoder. The proposed modeltransformed audio input into a gesture sequence in the form of3D joint coordinates. They achieved this by

- (i) learning a lowerdimensional representation of human motion using a denoising au-toencoder consisting of a motion encoder (called MotionE) and amotion decoder (called MotionD)

- (ii) training a novel speechencoder (called SpeechE) to transform speech to the correspondingmotion representation with reduced dimensionality. During infer-ence, the SpeechE predicted the motion representations, based ona given speech signal, and the MotionD decoded the motion repre-sentations into gesture sequences. However, their approach was de-terministic and thus unable to capture the commonly observed phe-nomena where a person gesticulates differently at different pointsof the same utterance.

Hasegawa và cộng sự [HKS∗ 18] đề xuất một phương pháp tự hồi quy để tạo ra cử chỉ từ âm thanh bằng cách sử dụng LSTM hai chiều [HS97]. LSTM hai chiều học được mối quan hệ âm thanh-cử chỉ với độ nhất quán tiến lùi và xuôi trong một khoảng thời gian dài. Mô hình được huấn luyện với bộ dữ liệu âm thanh-cử chỉ mới lạ, thu thập từ tai nghe và hệ thống ghi chuyển động dựa trên đánh dấu [TKS∗ 17]. Mô hình dự đoán tư thế toàn thân của con người từ đầu vào đặc trưng âm thanh ở mỗi bước thời gian LSTM. Sau đó, lọc thời gian được sử dụng để làm mịn đoạn chuyển động không liên tục.

Kucherenko và cộng sự [KHH∗ 19] mở rộng công trình của Hasegawa và cộng sự [HKS∗ 18], loại bỏ nhu cầu lọc thời gian thông qua việc học biểu diễn của bộ tự mã hóa. Mô hình đề xuất chuyển đổi đầu vào âm thanh thành chuỗi cử chỉ dưới dạng tọa độ khớp 3D. Họ đạt được điều này bằng cách

- (i) học biểu diễn human motion(động chuyển động con người) chiều thấp hơn bằng cách sử dụng bộ tự mã hóa giảm nhiễu (denoising autoencoder) gồm một bộ motion encoder (gọi là MotionE) và một bộ motion decoder (gọi là MotionD)
- (ii) huấn luyện một bộ mã hóa giọng nói mới (gọi là SpeechE) để chuyển đổi giọng nói thành biểu diễn chuyển động tương ứng với độ giảm chiều. Trong suốt quá trình suy luận, SpeechE dự đoán biểu diễn chuyển động dựa trên tín hiệu giọng nói đã cho, và MotionD giải mã biểu diễn chuyển động thành chuỗi cử chỉ. Tuy nhiên, phương pháp của họ là xác định và do đó không thể nắm bắt hiện tượng thường được quan sát là một người gesticulates khác nhau ở các điểm khác nhau của cùng một lời nói.

---

Deterministic generative approaches usually learn their param-eters using a regression objective, e.g. L1 (Mean Absolute Error)or L2 (Mean Squared Error). Optimizing with either of those ob-jectives typically forces the model toward learning to generate themean representation of the data, producing averaged motion for dif-ferent inputs, and resulting in undesirable results; usually called regression to the mean. Several approaches avoided this by incor-porating probabilistic components into their objectives. Probabilis-tic components can increase the range of gesture motion in mul-tiple ways, namely: (i) greater range of motion for different in-puts, or (ii) stochastic motion for the same input. The most promi-nent are implicit log-likelihood evaluation via adversarial learn-ing with Generative Adversarial Networks (GAN) [GPAM∗ 14],explicit log-likelihood evaluation via variational inference withVariational Autoencoders (VAE) [KW13], and exact log-likelihood evaluation via invertible transformations with Normalizing Flows[KPB20, PNR∗ 19].

GANs aim to do implicit density estimation of the underlyingdistribution through the interplay of a generator that tries to producesamples that are representative of the data, and a discriminator thatstrengthens the generator by classifying samples as real (from thedistribution) or fake (not from the distribution). Multiple gesturegeneration approaches added an adversarial objective as a term ina composite loss function, which increased the range of gesturemotion although still deterministic for a given audio input [SB18,GBK∗ 19,FNM20,YCL∗ 20,ALNM20,RGP21,WLII21b,WLII21a,ZRMOL22, HES∗ 22]. We discuss some notable examples below.Ferstl et al. [FNM19, FNM20] added multiple adversarial objec-tives to a recurrent neural network, known to be susceptible to re-gression to the mean for long sequences. The adversaries accountedfor gesture phase structure, motion realism, displacement and di-versity of the minibatch. Ahuja et al. [ALNM20] learned to implic-itly estimate a mixture of densities, each representing a speaker toenable the transfer of one speaker’s style onto the speech input ofanother. The mixture of densities was estimated by instantiating agenerator (per speaker) that is responsible for generating gesturesthat are representative of that speaker’s underlying gesture distribu-tion. All the generators were trained in unison using the adversarialobjective. Most recently, Habibie et al. [HES∗ 22] used an adver-sarial objective in the form of a conditional GAN to refine gestureclips that were selected using a k-Nearest Neighbour (kNN) searchthat is conditioned on speech and a control signal.Normalizing flows learn to describe highly complex distribu-tions by applying invertible sub-transformations to a simple ini-tial distribution, where invertibility allows one to optimize the ex-act likelihood of the deep generative model via gradient descent[KPB20,PNR∗ 19], unlike in GANs or VAEs. In the context of ges-ture generation, Alexanderson et al. [AHKB20] extended a normal-izing flow-based model for locomotion [HAB20] to apply to speechaudio-driven gesture synthesis with style control. Their normaliz-ing flows learned invertible transformations from simple Gaussiandistributions to the distribution of upper- or full-body motion cap-ture data. These transformations were conditioned on speech acous-tics and, optionally, arbitrary style parameters. Specific style pa-rameters considered included properties such as the average handheight, gesture speed, and gesture radius in a 4-second interval. Theresulting model produced gestures that scored among the best interms of naturalness and appropriateness in the GENEA Challenge2020 [KJY∗ 20].

VAEs aim to do explicit density estimation of the underlying databy optimizing a combination of a reconstruction loss for an autoen-coder (usually an L1 or L2 loss) and the Kullback-Leibler (KL)divergence for distribution matching between a prior distributionand approximate posterior distribution of the data. The prior distri-bution is usually instantiated as a Gaussian for simple parameter-ization. The learned stochastic variables can then be sampled anddecoded into diverse outputs. Recently, Li et al. [LKP∗ 21] useda conditional VAE to translate speech into diverse gestures. Theyexplicitly modeled a one-to-many speech-to-gesture mapping bysplitting the cross-modal latent code into a shared code (audio +gesture) and motion-specific code (gesture only). The shared codemodeled the correlation between speech and gesture e.g. synchro-nized speech and rhythmic gestures. The motion-specific code at-tempted to capture the diversity in gesticulation, independent ofaudio information.

In a similar vein, Ghorbani et al. [GFC22, GFH∗ ], used a VAE-based framework for style controllable co-speech gesture genera-tion conditioned by a zero-shot motion example i.e., an instance ofa motion style unseen during training. Given an audio input anda motion example, they generated an encoding of the audio and astyle embedding from the motion, and the two latent codes wereused to guide the generation of stylized gestures. The variationalnature of the style embedding enabled them to easily modify stylethrough latent space manipulation or blending and scaling of styleembeddings. Moreover, the probabilistic nature of the model en-abled the generation of varied gestures for any audio and exem-plar motion input. The resulting model performed favorably againststate-of-the-art probabilistic techniques [AHKB20] in terms of nat-uralness of motion, appropriateness for speech, and style portrayal.Taylor et al. [TWGM21] adapted the conditional Flow-VAEframework [BHF∗ 19], combining the advantages of the VAE andnormalizing flow architectures,to generate spontaneous gesturemovement for speaker and listener roles in a dyadic interaction.They used the Flow-VAE framework for modeling expressive ges-ture because of its ability to improve generative capacity of theVAE by estimating the latent space with a highly complex distri-bution using a normalizing flow, instead of the standard Gaussian.Their autoregressive framework was trained on a set of previouslygenerated gestures, an audio input window and the dyadic role, i.e.speaker or listener, as input. The preceding gestures were encodedinto a latent variable then transformed into a complex distributionusing a normalizing flow, conditioned by the audio window and rolein the dyad. Their decoder then generated the next gesture basedon the latent variable sampled from the complex distribution. Theresulting model could generate expressive co-verbal gestures in adyadic setting based.

Hybrid systems that combine deep learning and database match-ing components can also help tackle the regression to the meanproblem [KNN∗ 22, ZBC22, FNM21, HES∗ 22]. Indeed, this ap-proach has been used effectively in motion synthesis problems, e.g.game animation where high fidelity motion is crucial [HKPP20].In the context of conversational gesture, the intuition is that mod-eling the association between high dimension audio input and ges-tures, represented by exact joint positions or angles, using stan-dard regression objectives (L1 or L2 loss) discourages the modelfrom producing otherwise plausible gestures that do not exactlymatch the ground truth, thus greatly reducing the variety of gen-erated gestures. Alternatively, the audio-gesture association can bemodeled by predicting higher-level parameters for gesture motion.Ferstl et al. [FNM21] realized this idea by learning to map audio togesture via higher-level expressive parameters, specifically gesturevelocity, acceleration, size, arm swivel angle, and extent of handopening. First they pre-trained a model to associate audio prosodicfeatures to the expressive parameters. Then they predicted gesturetiming by extracting pitch peaks in the audio signal. At inferencetime, the prosodic features were used to estimate the expressive pa-rameters that were in turn used to search for a matching gesture inthe database, and the pitch peaks were used to temporally positionthe matching gesture. Finally, synthetic preparation and retractionphases were added to connect the gestures in the sequence.

Another interesting approach for preserving gesture form isthrough audio-based search in a video gesture database where thegestures are representated by video frames. Zhou et al. [ZYL∗ 22]explored this idea in a gesture reenactment task by generating agesture video for an unseen audio input, using gesture frames froma reference video. They first encoded the reference gesture videoas a “video motion graph” - a directed graph where each node rep-resented a video frame and corresponding audio features, and theedges represented transitions. The graph encoded how the refer-ence video can be split and re-assembled in difference graph paths.In order to increase graph connectivity, i.e, diversity of plausiblepath, they added synthetic edges based on a frame pose similaritythreshold computed using the SMPL pose parameters [LMR∗ 15].Given unseen audio input as a guide, they traversed the graph us-ing a beam search algorithm [RR77] to find the most optimal pathor order of gesture frames that best matches the speech audio. Forgraph paths that contain temporally disjoint frames, they trainedpose-aware video blending network to synthesize smooth transi-tions between the frames.

#### 5.3.2. Text input

Approaches that used audio as the primary modality producedwell-timed hand movements that tend to be highly associated withacoustics, largely corresponding to beat gestures. However, the lackof text transcript means they were not informed by the structureand context inherent in the text, for example, semantic meaningand punctuation. Such structure can help produce more meaning-ful and communicative gestures. Therefore, next, we describe someapproaches that used text as the primary input modality.

Ishi et al. [IMMI18] proposed a text-based gesture generation ap-proach for controlling a humanoid robot. They modeled the text-to-gesture motion translation by associating words to concepts, con-cepts to gesture categories (i.e. iconic, metaphoric, deictic, beat,emblem and adapter), and gesture categories to gesture motions.Further, they estimated conditional probabilities to model the as-sociation between word concepts and gesture categories, and be-tween gesture categories and gesture motion clusters that were pre-computed with the k-means clustering algorithm.

Yoon et al. [YKJ∗ 19] proposed an encoder-decoder approachthat transformed speech text, from a dataset based on TED talks,into a sequence of gestures. They created the TED video dataset bypicking video segments with the speaker’s upper body and hands.Then they performed pose estimation using OpenPose [CHS∗ 18]and removed segments containing noisy or no estimations. Speechtext was converted into a sequence of 300-dimensional wordvectors using pre-trained GloVe embeddings [PSM14]. Similarly,poses estimations were converted to 10-dimensional vectors us-ing principal component analysis (PCA). Therefore, co-speech ges-ture generation became a sequence-to-sequence translation prob-lem from word embeddings to human poses encodings. The en-coder part of the network was a bi-directional GRU [CVMG∗ 14]taking in speech text one word (vector) at a time and capturing bi-directional context. The last hidden state of the encoder was passedinto the decoder, also a bi-directional GRU. The decoder also tookprevious pose estimations to condition the prediction of the nextpose, in addition to using soft-attention [BCB14] to focus on spe-cific words when predicting the next pose. Finally, the generated2D poses were mapped to 3D and executed on the NAO humanoidrobot.

Recently, Bhattacharya et al. [BRB∗ 21] used text transcripts toproduce expressive emotive gestures for virtual agents in narra-tion and conversation settings, using MPI-EBEDB, a dataset ofactors performing multiple emotion categories (amusement, anger,disgust, fear, joy, neutral, pride, relief, sadness, shame, surprise)[VDLRBM14]. Their approach consisted of Transformer-based en-coders and decoders [VSP∗ 17], where the encoder took in the texttranscript sentences (encoded as GloVe embeddings [PSM14]) toproduce an encoding which was concatenated with the agent at-tributes such as narration/conversation, intended emotion, genderand handedness. The previous pose’s encoded concatenation and3D joint positions were passed as input to the Transformer de-coder to generate the next pose’s joint positions. The process wasrepeated in a recurrent manner until the full pose sequence wasgenerated.

#### 5.3.3. Audio and text input

An interesting trade-off exists between audio-based and text-basedgesture generation systems. audio-based generators have access tointonation and prosody which helps generate rhythmic or kine-matic gestures (e.g. beats) but lack semantic context. Conversely,text-based generators have access to semantic context which helpsgenerate meaning-carrying gestures (e.g. iconic or metaphoric), butlack intonation and prosodic information. Therefore, combining theaudio and text modalities enables a gesture generator to learn toproduce semantically relevant and rhythmic co-speech gestures.Although generating meaning-carrying gestures using audioonly is theoretically possible, it is unlikely since prosody is suitablefor kinematics, but not sufficient to infer shape which is associatedwith meaning [LWH∗ 12]. As far as we know, meaningful gesturesfrom speech audio alone have not been empirically demonstrated.Instead, combining audio with text appears to be the most promis-ing approach to generating meaningful gestures to date. We, there-fore, focus on approaches that combine these two modalities forgenerating meaning-carrying, communicative gestures.

Chiu et al. [CMM15] proposed an approach that combined thetext and prosody of the speech to generate co-verbal gestures. Theirmodel, called the Deep Conditional Neural Field (DCNF), was acombination of a fully-connected network, for representation learn-ing, and a Conditional Random Field (CRF), for temporal model-ing. For the gesture prediction task, the model took in a text tran-script, part-of-speech tags and prosody features as input, and pre-dicted a sequence of gesture signs which were a set of predefinedhand motions.

Leveraging the representation power of deep learning models formultimodal input (i.e. audio and text) for co-speech gesture gener-ation was the next logical step. In fact, three groups of researchersindependently proposed the first deep-learning-based gesture gen-erators that used both audio and text to generate continuous ges-tures, namely Yoon et al. [YCL∗ 20], Ahuja et al. [ALIM20] andKucherenko et al. [KJvW∗ 20]. We discuss their pioneering workcombining audio and text next, followed by subsequent efforts inthe area.

Yoon et al. [YCL∗ 20] proposed a gesture generation approachthat combines the tri-modal context of speech, text and speakeridentity to produce gestures that were human-like, and matchedthe content and rhythm of the speech. The model processed in-put speech and text with a speech and text encoder, respectively.The speaker identity was used to sample the intended speaker froma learned style embedding space. Together the three features (i.e.speech encoding, text encoding, and style) were passed to a ges-ture generator to produce the sequence of poses. Closely relatedwas Liang et al. [LFZ∗ 22] whose framework utilized audio andtext information in order to generate meaningful gestures by disen-tangling semantic and beat gestures. Their system consisted of twoencoders, one that took in audio and text to encode semantics, andanother that took in audio volume and beat to encode non-semanticinformation. The encoded information from both encoders ensuredthe disentanglement of semantic and beat gestures, while decodertook this information and was trained to encourage generation ofmeaningful semantic gestures.

Ahuja et al. [ALIM20] identified two key challenges in an at-tempt to learn the latent relationship between speech and co-speechgestures. First, the underlying distributions for text and gesture areinherently skewed and therefore necessitated the need to learn theirrespective long tails, accounting for rarely occurring text or ges-tures. Second, gesture predictions are made at the sub-word level,which necessitated the need to learn the relationship between lan-guage and acoustic cues that may give rise to, or be accompaniedby, a particular gesticulation. So motivated, they proposed the Ad-versarial Importance Sampled Learning (AISLe) framework, thatcombined adversarial learning with importance sampling to bal-ance precision and coverage. The model took in speech and texttranscripts and performed encoding and alignment between sub-words and acoustics, using a multi-scale Transformer [VSP∗ 17].The resulting alignment was passed to the model’s generator to pre-dict the pose sequence and an adversarial discriminator was used todetermine if the pose was real or fake. For optimizing the adversar-ial objective, the AISLe framework scaled the loss function suchthat rarely occurring gesture samples, the long tail of the distribu-tion, were weighted more than those that are more likely to occur.

Kucherenko et al. [KJvW∗ 20] proposed an autogressive gen-erative model that combined speech acoustics and semantics toproduce arbitrary acoustically-linked or semantically-linked ges-tures. The key insight of their approach was to envision a gestic-ulation system that encompasses so called “representational” ges-ture types (i.e. iconic, metaphoric and deictic) that convey seman-tics, and beats that are synchronized with acoustics. Their approachtook a concatenation of semantic features that were extracted usingBERT [DCLT18] and acoustic features represented as log-powermel-spectograms as input into an encoder. Then they integrated pastand future context for each gesture pose frame via a sliding windowoperation over the encoded speech features. The model generatedeach pose autoregressively where each was conditioned on the in-formation of three preceding frames to ensure motion continuity.Their extensive evaluation indicated that autoregression for contin-uous motion and combining audio and text had the most significantpositive impact on the quality of the generated gesticulations.

Equally inspired by autoregressive generative models, Korzun etal. [KDZ21, KDZ20] reimplemented the text-only recurrent frame-work by [YKJ∗ 19] to accommodate both text and audio input. Theproposed model was a combination of a recurrent context encoder,inspired by [KHH∗ 19], that generated hidden states for 3-secondaudio and text context windows and a recurrent encoder-decoderthat took in the concatenated results of the context encoder andused an attention mechanism to condition the generation of the fi-nal gesture motion. Similar to Yoon et al. [YKJ∗ 19], they trainedthe model using the continuity and variance objectives to ensurefluid and natural-looking gestures. The resulting model producedgestures that were deemed natural and appropriate as part of theGENEA Challenge 2020 [KJY∗ 20].

Designing generation systems that produce meaningful gesturesis one of the major goals in non-verbal behavior research. Spurredon by this question, Kucherenko et al. [KNN∗ 22] investigatedwhether contemporary deep learning-based systems could predictgesture properties, namely phase, type and semantic features, asa way to determine if such systems can consistently generate ges-tures that convey meaning. Their model used both audio and text forpredicting gesture properties, through two distinct components thatpredicted the probability for gesticulation and probabilities for theaforementioned set of gesture properties. They conducted their ex-periments on a direction-giving dataset with a high number of rep-resentational gestures [LBH∗ 13]. Their experiments showed thatgesture properties related to meaning such as semantic propertiesand gesture type could be predicted from text features (encoded asFastText embeddings [BGJM17]), but not from prosodic audio fea-tures. Conversely, they found that rhythm-related gesture properties(e.g. phase) could be better predicted from audio features.

In order to mimic the communicative intent of co-speech ges-tures, it is crucial to understand and model the complex relation-ship between speech acoustics, text, and hand movements. An in-teresting approach is to group gestures with distinct movementproperties in order to find emergent rhetorical categories. Saundet al. [SBM21] investigated this approach by modeling the rhetor-ical, semantic, affective and acoustic relationships between ges-tures and co-occurring speech audio and text, for a hypotheticalgesture generation system. They first used k-means clustering tocluster speech-gesture pairs into functional domain clusterings (i.e.rhetorical, affective and semantic) based on functional tags gener-ated from third-party natural language parsers. The speech-gesturepairs were refined into sub-clusters based on gesture motion. There-fore each speech-gesture pair belonged to at least one sub-cluster(based on motion), within one functional cluster (based on its as-signed functional tags). At run-time, a hypothesized virtual agentwould leverage the same pre-trained parsers and clusters to analyzean input speech and text transcription, select a functional clusterand from that a motion sub-cluster. The agent could then eitherchoose an appropriate gesture from a pre-recorded library or thecentroid gesture in the motion sub-cluster.

Motion graphs, commonplace in conventional animation sys-tems (e.g. [KGP02, AF02, LCR∗ 02]), can be effective at produc-ing realistic non-verbal behavior because they rely on databasesof high-quality motion capture or RGB video. As we discussedbefore, they were effectively employed for audio-driven gesturereenactment using video-based motion graphs [ZYL∗ 22]. Zhou etal. continued this trend for audio and text by adapting a motion-graph-based music-to-dance system [CTL∗ 21] for co-speech ges-ture generation [ZBC22]. They first built a database of audio, textand gesture clips from 3-tuples of (audio, text transcript, gesture),using a splitting algorithm. For each audio clip, they generated astyle signature using StyleGestures [AHKB20], and a rhythm sig-nature using a binary encoding scheme that denotes the presence ofwords by leveraging the word-level timing information in the texttranscript. For the corresponding gesture motion, they generated astyle signature, parameterized by the same attributes as StyleGes-tures [AHKB20] (e.g. wrist speed, radius and height), and a rhythmsignature using a similar binary scheme that denoted the presenceof pausing, sharp turning or a stroke phase of the gesture. Duringsynthesis, they computed the rhythm and style signatures for in-put audio and text and used a graph-optimization algorithm to findgesture clips that closely matched the generated style and rhythmin terms of Hamming distance, and minimized the motion transi-tion in the graph. This model performed on par or better than mo-tion capture data in terms of Naturalness in the GENEA Challenge2022 [YWK∗ 22].

Style transfer is a widely adopted optimization technique in deeplearning for blending visual content and style, e.g. given a con-tent image and a reference image that specifies the style, adjustthe content image to match the style [GEB15]. In the context ofco-speech gesture generation, it might be desirable to transfer thespeaking style of one speaker to the predicted gestures of another.Ahuja et al. [ALNM20] learned unique style embeddings for mul-tiple speakers that enabled either generation of gestures consistentwith the original speaker in the audio input, or style transfer bycombining the audio input of one speaker with the style embed-ding of a different speaker. Although they proposed the PATS datawhere multiple modalities such as audio, gesture pose and text havestyle and content, they focused on gesture pose style to learn uni-modal speaker-specific style embeddings. Fares et al. [FGPO22]leveraged the multiple modalities in the PATS dataset to learn mul-timodal style embeddings based on audio, text and gesture poseinput. Their framework consisted of a speaker-style encoder thatused speaker audio, text and gesture pose to learn a multimodalstyle embedding, and a sequence-to-sequence decoder that gener-ated gestures based on audio and text, and conditioned on the de-sired speaker’s style embedding. Furthermore, unlike the work ofAhuja et al. [ALNM20] that required the entire speaker’s gesturedata to learn the speaker’s style embedding, their trained speaker-style encoder could generate style embeddings in a zero-shot man-ner i.e., for speaker styles not seen in the training set.

A key tenet of semantically meaningful gestures is that they areappropriate for the given utterance. To achieve this, there needs tobe a greater emphasis on generating precise gestures using audioand text as grounding (i.e. the appropriateness of the gesture to theutterance), versus generating diverse gestures. Lee et al. [LAM21]investigated this approach and made an interesting observationabout human gesticulation, that multiple semantically different ut-terances are often accompanied by the same gesture. They thus pro-posed a contrastive-learning framework that constrained the map-ping of semantically different utterances to a smaller subset of rel-evant high-quality gestures. They introduced a novel contrastivelearning objective that preserved similarities and dissimilarities ofgestures in the latent representation. The objective ensured that la-tent language representations of two semantically different utter-ances were close together if they were accompanied by the samegesture. They first clustered gestures based on similarity or dis-similarity, then created positive (similar gesture poses) and nega-tive (dissimilar gesture poses) required for the standard contrastivelearning objective. Finally, they learned gesture-aware embeddingsvia a contrastive and adversarial objective. The resulting embed-ding space was used to generate gestures that were semanticallyrelevant and closer to the ground truth.

When designing 3D avatars, it may be desirable to have a holis-tic animation system that includes facial and full-body movement.Combining audio and text modalities can be effective at achiev-ing this goal because of their rhythmic and semantic properties.Zhuang et al. [ZQZ∗ 22] investigated this approach by proposinga hybrid system consisting of Transformer-based encoder and de-coder modules, and motion-graph retrieval module to generate fa-cial motion and full-body motion that included gestures. Their en-coder used both audio and text, in the form of phoneme labels andMel Frequency Cepstral Coefficients (MFCC) and Mel Filter Bank(MFB) features, to generate 3D facial parameters for synchronouslip movement. Simultaneously, the decoder used speech features,previous expression motion and semantic tags to generate 3D facialparameter for expression. The motion-graph retrieval sub-systemused speech audio and text to find the most appropriate body mo-tion segments, including gesture, that correspond to the text seman-tics and rhythm in the audio. Finally the facial and body motionwere used to drive a skinned polygonal model.

#### 5.3.4. Non-linguistic modalities

Several deep learning-based systems complemented input audio ortext with additional information that could reasonably be deemedrelevant to co-speech gestures. This included speech context,speaker style, discourse or an interlocutor’s movements. Sadoughiand Busso [SB19] proposed a system that bridges rule-based andlearning-based techniques in order to select gestures that are com-municative and well synchronized with speech. They proposed aDynamic Bayesian Network (DBN) which took in speech and twoconstraints to condition the generation. The constraints were: 1)discourse function, which restricts the model to behaviors that arecharacteristic of that discourse class (e.g. questions); 2) prototypi-cal behaviors, which restricted the model to certain target gesticula-tions (e.g. head nods). Given constraints on prototypical behaviors,the approach could be embedded in a rule-based system as a behav-ior realizer creating head and hand trajectories that are temporallysynchronized with speech.

In a dyadic conversation between interlocutors, there can be alot of spontaneous non-verbal behavior that is influenced by thenature and tone of the interaction. Leveraging the co-adaptationof non-verbal behavior between interlocutors present in human-to-human interactions, cf. [BK12, CBK14, OB16], can enable vir-tual agents to be naturally conversational and collaborative. Ahujaet al. [AMMS19] proposed the Dyadic Residual-Attention Model(DRAM), a framework that could interactively generate an avatar’sgesticulation conditioned on its speech and also the speech and ges-ticulation of a human interlocutor in a telepresence setting. In or-der to generate natural behavior, the avatar had to consider its ownspeech as well as the speech and gesticulation of the human. TheDRAM model generated natural dyadic behavior by taking in thespeech and pose history of the avatar as well as the speech and posehistory of the human to adapt the avatar’s gesticulation accordingly.The idea of conditioning the motion of a deep-learning-basedagent on interlocutor speech and motion has subsequently beenused in several other works. Jonell et al. [JKHB20] used a modelbased on normalizing flows for generating head motion and facialexpression, while Nguyen and Celiktutan [NC22] used conditionaladversarial learning to drive full-body skeletons. Both of theseworks found that statistically significant improvements in gener-ated behaviors were achieved by being interlocutor-aware.

A similarly interesting dyadic scenario is human-robot interac-tion where one of the interlocutors is a social robot. In this case,the robot must exhibit natural non-verbal behavior in order to beengaging and interesting. Therefore, it is desirable for the robot tomimic human non-verbal motion with gestures that are natural andcommunicative. Deichler et al. [DWAB22] investigated this idea byproposing a combination of a data-driven and physically-based re-inforcement learning (RL) framework to generate pointing gestureslearned from motion capture data. Given a diverse motion capturedataset of pointing gestures and corresponding targets, they trainedRL control policies adapted from [PALvdP18, PMA∗ 21] to imitatehuman-like pointing motion while maximizing the reward based onpointing precision.

Automatic synthesis and animation of gestures that accompanyaffective verbal communication can endow virtual agents withemotional impetus. Bozkurt et al. [BYE20] directly mapped emo-tional cues in speech prosody into affect-expressive gestures. Theyinvestigated the use of three continuous affect attributes (i.e. ac-tivation, valence and dominance) for the speech-driven synthesisof affective gesticulation. They proposed a statistical model basedon hidden semi-Markov models (HSMM) where states were ges-tures, and observations were speech prosody and continuous af-fect attributes. They first estimated the affective state from speechprosody and then used the state and speech prosody to predict ges-ture clusters. The gesture segments were animated using a unit se-lection algorithm [BEY15], and discontinuities were smoothed us-ing an exponential smoothing function. Finally, the smoothed se-quence was animated in Autodesk MotionBuilder.

Text encodes important semantic information, potentially use-ful for conveying meaningful emotion through gesture, althoughit encodes fewer cues about emotional state compared to audioe.g., intonation and speech pauses. An interesting approach is tocombine text with an intended emotion for affective gesture gen-eration. Bhattacharya et al. [BRB∗ 21] pursued this approach bycombining text transcripts associated with narrative or conversa-tional acting and emotion labels, to produce expressive emotivegestures for virtual agents. The emotions represented were amuse-ment, anger, disgust, fear, joy, neutral, pride, relief, sadness, shameand surprise [VDLRBM14]. Their approach consisted of a Trans-former [VSP∗ 17] encoder and decoder, where the encoder took inthe text transcript sentences, intended emotional state and agentattributes (e.g. narration/conversation, intended emotion, gender,handedness). The previous pose’s encoded concatenation and 3Djoint positions were passed as input to the Transformer decoder togenerate the next pose’s joint positions. The process was repeatedin a recurrent manner until the full affective pose sequence wasgenerated.

A speaker’s identity or style can affect how they gesticulate, assome speakers gesture a lot while others rarely do. Moreover, theymay also prefer particular gesture forms, and use different hands orgesture sizes. Modeling such variation in non-verbal behaviour canhelp make virtual agents seem unique and have a personality. Tothis end, Yoon et al [YCL∗ 20] used speaker identity to guide ges-ture generation that matched the speaker’s style. Their adversarialapproach combined the tri-modal context of audio, text and speakeridentity to produce gestures that were human-like, and matchedthe content and rhythm of the speech. The model processed in-put audio and text with an audio and text encoder, respectively.The speaker identity was used to sample the intended speaker froma learned style embedding space. Together the three features (i.e.audio encoding, text encoding, and style) were passed to a ges-ture generator to produce the sequence of poses. Similarly, Ahujaet al. [ALNM20] learned a mixture of adversarial generators, rep-resenting diverse gesticulation styles of speakers from talk-showhosts, lecturers and televangelists. Learning speaker-specific gener-ators enabled one speaker’s style to be aligned with, or transferredto, the audio of another speaker.

Developing robust deep-learning-based gesture generators re-quires large amounts of diverse gesture data from real world sce-narios, captured either via motion capture or pose estimation fromvideos. However, capturing or estimating hand gestures is verychallenging because of the intricate finger motion, relatively smallsize of hands with respect to the whole body and frequent self-occlusions [HLW∗ 18, LDM∗ 19, JYN∗ 20]. In contrast, capturingbody motion (up to and including the arms) is less error pronebecause the joints are further apart and the articulations are rel-atively simpler. Therefore, the “upper body” motion as a modal-ity can be an informative prior for generating conversational handgestures. Ng et al. [NGDJ21] investigated this idea while makingthe observation that body motion is highly correlated with handgestures. Their proposed approach took in 3D upper-body mo-tion (up to the wrist) and predicted 3D hand poses. In addition toupper-body motion, the model could take in 2D images of handsand produce the corresponding 3D hand pose estimations. Simi-lar to [GBK∗ 19], they used a combination of a L1 regression lossfor the model training signal and an adversarial loss to ensure re-alistic motion. The learned body-motion-to-hands correlation wasversatile enough for several use-cases, namely conversational handgesture synthesis, single-view 3D hand-pose estimation and syn-thesizing missing hands in motion capture data and image-basedpose estimation data.

#### 5.3.5. Control input

Although control can take either linguistic or non-linguistic forms,it is distinct because it can convey the explicit design and ex-ecution intent of an animator. Multiple works in motion syn-thesis use control as an additional input either during the train-ing phase or the inference phase of learning-based models (e.g.[HKS17, LZCvdP20]). Typically, during training, the control sig-nal is used to train the system to generate animations with cer-tain biomechanical constraints such as posture, gait, etc. Duringinference, control may be introduced to impose style-related con-straints [SCNW19] or user input [HKS17, HAB20, LZCvdP20].

In the context of conversational gesture, Alexanderson etal. [AHKB20] trained a probabilistic model that generated sponta-neous co-verbal gesture that was conditioned on control constraintssuch as wrist height, radial extent and handedness. However, theconstraints were introduced at training time, meaning modelinga new constraint required re-training the entire model. Habibie etal. [HES∗ 22] provided a more flexible approach. They first learna speech-to-gesture motion search through a kNN algorithm, andthen refine the motion using conditional GAN. Style control canbe exerted at runtime by dynamically restricting the portion of thedatabase that the kNN algorithm is run on, allowing style variationeven within an extended utterance without the need to retrain.Control can also be imposed by implicitly specifying the de-sired gestures by learning emergent prototypes of gesture shape orform. Qian et al. [QTZ∗ 21] explored this idea by learning condi-tional vectors, so-called “template vectors”, that could determinethe general appearance and thus narrow the potential range of plau-sible gestures. Their framework took in audio and a zero initializedcondition vector, through a 1D UNet-based autoencoder, in orderto generate the corresponding gestures as 2D joint positions. Dur-ing training, they periodically updated the condition vector, throughback-propagation, using the gradients computed on the L1 regres-sion loss between the generated and ground-truth gestures. Theyregularized the template vector space through the KL-divergencebetween the vectors and a normal distribution. They also separatelypre-trained a VAE to reconstruct ground truth gestures and used theresulting latent space to encode gestures into template vectors. Attest time, they sampled arbitrary template vectors, either learnedthrough back-propagation or extracted by the pre-trained VAE, togenerate diverse gestures.

Animators typically want to specify high-level style parametersto convey design intent e.g. energetic oratory gesticulations or sub-dued gestures to convey sadness. Additionally, it is desirable tospecify the style once in the workflow and for the animation systemto generate arbitrarily many motions for that specification. How-ever, there is a gap between desired abstract design intent and ex-isting deep-learning-based style control systems that tend to relyon biomechanical constraints such as wrist speed, radius or height[AHKB20, HES∗ 22]. Style specification is also not data efficient,requiring as many samples as the size of the training set for themodel to learn a style [AHKB20,ALNM20]. We conclude this sec-tion by discussing several works that proposed approaches for data-efficient style specification [GFC22, GFH∗ , FGPO22, ALM22].

Ghorbani et al. [GFC22, GFH∗ ] proposed a framework that im-proves on high-level style portrayal by using exemplar motion se-quences that demonstrate the intended stylistic expression of ges-ture motion. Their framework was able to efficiently extract styleparameters in a zero-shot manner, only requiring a single examplemotion and was able to generalize to example motions (and there-fore styles) unseen during training. Fares et al. [FGPO22] used anadversarial framework to learn a speaker-style encoder that couldgenerate speaker-specific style embeddings from novel multimodalinputs – audio, text and gesture pose – not seen during the trainingphase. The framework generated co-speech gestures in a style thatis either consistent with the original speaker in the audio or a dif-ferent speaker, depending on the chosen style embedding. Ahuja et.al [ALM22] proposed an adversarial domain-adaptation approachfor personalizing the gestures of a source speaker with plenty ofdata, with the style of a target speaker with limited data, usingonly 2 minutes of target training data. Given a model pretrainedon a large co-speech gesture dataset, their framework could adaptthe model’s parameters using a smaller target dataset by modelingthe cross-modal grounding shift, i.e., the change in distribution ofspeech-gesture associations, and the distribution shift in the targetgesture space. The approach’s ability to identify distributions shiftsbetween the source and target domain for parameter updates, en-abled the model to extrapolate to gestures in the target distributionwithout having seen them in the source distribution during pretraining.

## 6. Key Challenges of Gesture Generation

Animating co-verbal gestures is still a very challenging problembecause gestures are spontaneous, highly idiosyncratic and non-periodic. Rule-based approaches generate well-formed gestures byleveraging recording motion, but are inflexible and lack gesture di-versity. Additionally, the hand-designed rules are non-exhaustiveand often prescriptive, and hence may not be reflective of gestureswhich occur naturally and spontaneously. Data-driven approachesimprove on diversity and flexibility but tend to produce marginallynatural gestures that appear more like well-timed hand waving, arenot communicative and have little meaning. Although state-of-the-art systems employ speech and/or text information, they still donot handle semantic grounding of gestures properly, evidenced bygestures that seem to lack meaningful information when comparedto the ground truth. Furthermore, due to the probabilistic nature ofgestures, its idiosyncrasies, rich semantic content makes the evalu-ation process especially challenging and subjective. In this section,we discuss the limitations of the current work and possible futuredirections in context of what we view as the key challenges of ges-ture generation, namely:

1. evaluation (in Section 6.1),
2. data (in Section 6.2),
3. human-like gestures (in Section 6.3),
4. multimodal grounding (in Section 6.4), and
5. multimodal synthesis (in Section 6.5).

### 6.1 Evaluation

Evaluation is of central importance to gesture generation, both fordeveloping co-speech gesture generation systems and for assess-ing their performance and capabilities in various aspects, as wellas those of the field as a whole. However, evaluating gestures ischallenging due to the stochastic nature of gestures and the highlysubjective nature of human gesture perception. A comprehensivereview of evaluation practices in gesture generation can be foundin [WRB22]. We recommend that readers consult that review re-garding best practices, but also provide an overview of key openchallenges in gesture evaluation here.

#### 6.1.1. Subjective Evaluation

One important aspect to evaluate for gesture-generation systems isthe human-likeness of the generated gestures, which is measuredand compared through human perceptual studies, often with com-parable stimuli presented side by side as in e.g. [JYW∗ 21,KJY∗ 21,WGKB21]. On the other hand, evaluating the other aspects suchas the appropriateness and/or specificity of generated gestures inthe context of speech and other multimodal grounding informa-tion (see Section 6.4) is quite challenging, especially since differ-ences in the human-likeness of the motions being compared tendsto interfere with perceived gesture appropriateness (cf. the resultsin [KJY∗ 21]). To alleviate this challenge for appropriateness, a newevaluation paradigm of matched vs. mismatched gesture motion hasrecently been proposed [JKHB20,RGP21,YWK∗ 22]. In this setup,human participants are asked to choose between two motion clipsthat both were generated by the same system, and therefore havesimilar appearance and human-likeness, but where one clip is in-tended to be appropriate to the situation (e.g., the motion in it cor-responds to the actual speech audio in the video) whereas the otheris chosen at random (e.g., it was generated by feeding unrelatedspeech audio into the same system instead, and does not match theactual audio track). The extent to which humans are able to iden-tify the video that matches the situation can be used both to probethe strength of grounding in different modalities, and to assess ges-ture appropriateness for speech, rhythm, interlocutor behavior, etc.,while controlling for human-likeness. We expect this methodologyto gain wider adoption and advance the state of the art in subjectiveassessment of different aspects of co-speech gestures.

Another compelling area for future work is to evaluate gesturegeneration in actual interactions, since the ultimate goal of em-bodied conversational agents is to enhance human-computer com-munication and interaction. Initial studies [NKM∗ 21,HPK22] havefound that embodied agents that perform gestures generated by datadriven models as opposed to performing no gestures, attract moreattention from the audience. A larger attention span on a gesticu-lating agent is indicative of a more engaging communicative qual-ity of gestures and opens doors to evaluating gesture generation ina more natural setting. Although the situated and time-demandingnature of such interactions, coupled with their reliance on manynon-gesture components necessary to create interactivity (e.g. hu-man wizards or automatic speech recognition, dialogue systems,and text-to-speech), make proper interactive evaluation challeng-ing and seldom done, it is an important long-term goal for eval-uations in the field. Given the difficulties in comparing differentresearch papers in the field, we think that controlled, large-scalecomparisons [KJY∗ 21, YWK∗ 22] with open data and materials aregoing to play an important role to develop the co-speech gesturefield and its evaluation practices in the shorter term. This is simi-lar to the role challenges have played in the development of text tospeech [Kin14] and the wide use of leaderboards and benchmarksacross deep learning today.

#### 6.1.2. Objective Evaluation

While subjective metrics from appropriately designed user-studiesare the gold standard in co-speech gesture evaluation [WRB22],they are expensive and time consuming, and thus lack scalability.There is therefore interest in objective metrics to automatically as-sess synthetic motion, for example its its human-likeness. Objectivemetrics are useful to measure progress during model developmentin a heavy compute, data-driven learning setup. A natural metricis accuracy of prediction (i.e., how often the predicted position ofa joint is within some tolerance of the joint position in a humanmotion capture clip), which is often called the Probability of Cor-rect Keypoints (PCK). However, this quantity is often not indica-tive of performance due to the one-to-many nature of the gesture-generation problem. Two examples of human motion for the samespeech might involve very different joint positions, and thus havelow mutual agreement. Measuring the mean squared error (MSE)between generated motion and human motion capture suffers fromthe same issue.

Statistics of motion properties such as acceleration and jerk havebeen used as an alternative for quantifying and comparing gener-ated gesture distributions [KHK∗ 21], but there is no compellingevidence that these metrics correlate with subjective assessmentsof motion human-likeness. To improve the measurement of distri-butional similarity of gestures, new objective quality metrics basedon innovations from image processing, namely the Fréchet Incep-tion Distance (FID) [HRU∗ 17] and the Inception Score [SGZ∗ 16],were proposed in [ALIM20,YCL∗ 20] and [ALNM20] respectively.Among these proposals, only [YCL∗ 20] computes the Fréchet dis-tance in a learned space. There has also been work in learning toestimate the human-likeness of gestures from databases of gesturemotion and associated subjective ratings data [He22]. However,learning to predict human preference can be difficult even fromrelatively large training databases, as seen in similar research intopredicting the subjective ratings of synthetic speech [HCT∗ 22].Since the above approaches depend on motion data only, theycan only give an indication of whether or not generated motion isstatistically similar to the human motion capture in the database,but not how appropriate the motion is for the context in which itoccurs (whether it is grounded in that context). The methods cantherefore not assess whether or not the motion is synchronized withthe co-occurring speech, whether the motion is semantically rele-vant, etc. In general, unlike human-likeness, not many techniqueshave been proposed for objectively quantifying properties like ges-ture diversity or different kinds of motion appropriateness. One ex-ception is the recent Semantic Relevance Gesture Recall (SRGR)metric from [LZI∗ 22], which proposes to quantify the semantic rel-evance of gesture by using semantic scores, annotated in the speechtext data, to weight the probability of correct keypoints betweenthe predicted and ground-truth gestures higher when the ground-truth gesture has a high semantic score. This is a step in the rightdirection for evaluating semantic appropriateness, but may sufferfrom the same issues as regular PCK due to the idiosyncratic, one-to-many nature of gesticulation. Given the impact that the Incep-tion Score and the Fréchet Inception Distance have had in drivingprogress in image generation, reliable metrics that estimate gesturehuman-likeness and especially appropriateness for e.g. the rhythmand semantics of co-occurring speech are an important continuingchallenge, where recent and future innovations are likely to havesignificant impact on the field.

### 6.2. Data

Compared to machine-learning applications in text, speech, and im-ages, gesture-generation is currently a data-limited field. A partic-ular bottleneck is finger motion, which is difficult to capture accu-rately even through motion capture; cf. Table 1. When finger mo-tion is unreliable or unavailable, a possible mitigation might be topredict finger motion from other information, for example the restof the body as in [NGDJ21]. In general, motion capture data ishigh quality, but laborious to capture, particularly when consideringlarge scale data corpora. Other issues arise due to the high variationin gesture behavior. It can vary based on the individual, the environ-ment, the number of people interacting, their emotional state andthe topic of the conversation. Some of this variation is grounded ininformation that cannot be effectively recorded because it, e.g., isinternal to a speaker (such as their emotional state), or that is rarelycaptured, such as properties of the space in which an interaction istaking space. But even if one were to capture or control for many ofthese these sources of variation, a great diversity in gesture behav-ior and realization would persist, which will be difficult to cover inany database we can record.

In the long term, if we can achieve sufficiently reliable 3D ges-ture extraction from monocular, in-the-wild online video, that willbe a game-changer for the field of co-speech gesture generation. Itpromises to have a transformative impact on both perceived authen-ticity and model capabilities, similar to how very large datasets fordeep learning has powered recent advances in generative modelsfor text and images, such as GPT-3 [BMR∗ 20], DALL-E [RPG∗ 21,RDN∗ 22], and Stable Diffusion [RBL∗ 22]. At present, works thatstudy the use of in-the-wild data for gesture synthesis exist, for ex-ample [YKJ∗ 19, GBK∗ 19, YCL∗ 20, HXM∗ 21, ALIM20], but thequality of the data and the gestures do not yet amount to such aleap forward.

### 6.3. Human-Like Gestures

The most prominent research target in deep-learning-based co-speech gesture generation has long been perceptual quality. This issimilar to the focus on perceptual surface quality in other areas suchas image generation [KLA∗ 20, RPG∗ 21, RDN∗ 22, RBL∗ 22] andspeech synthesis [WSRS∗ 17, vdODZ∗ 16]. One reason for this fo-cus might be that perceptual surface quality is easier to estimate us-ing standardized procedures, compared to quantities such as “ges-ture appropriateness for speech”. See, especially, the rapid qualityimprovements in the image-synthesis field, once reasonable objec-tive metrics such as the Inception Score [SGZ∗ 16] and the FréchetInception Distance [HRU∗ 17] became available.

Just like deep generative methods in general have advancedgreatly in recent years, there is strong evidence from large eval-uations that the human-likeness of the best gesture-generation sys-tems is improving as well [KJY∗ 21, YWK∗ 22]. The better thevisual quality of the avatar and greater range of expressive mo-tion, the easier it should be to spot differences between naturaland synthetic motion. From this perspective, head motion (whichonly has three degrees of freedom) might for example be eas-ier to make indistinguishable from human head motion, than it isto generate convincing arm and finger motion. In this light, theachievement of GestureMaster [ZBC22] in the GENEA Challenge2022 [YWK∗ 22] is particularly noteworthy, since the synthesizedupper- and full-body gestures produced by this model were ratedhigher than the original motion capture from the human speaker.Although a very impressive result, this may partly be attributed tothe presence of some motion clips with motion-capture artifacts,especially for the fingers, that may reduce the perceived human-likeness of the notional human reference motion.

At the same time, even “high quality” gesture motion on a high-fidelity avatar is still judged as being far from human: in the GE-NEA Challenge 2022 [YWK∗ 22], neither the human motion cap-ture nor the best performing system came near the rating of 100 thatwould correspond to being “completely human-like”. More specif-ically, the median human-likeness of the best performing synthesissystem were 69 for upper-body motion and 71 for full-body mo-tion, with scores of 63 and 70 for human motion capture, respec-tively. Our statement comes with several caveats. Some of the gapup to a score of 100 might be attributable to shortcomings of mo-tion capture when it comes to capturing the full range of humanexpression. For example, how an avatar moves and its lack of face,mouth, gaze and lip motion behavior can impact the visual qualitiesof the avatar. Even in the case of speech synthesis, where recreatinghuman behavior is as easy as playing back an audio recording, it iswell known that humans tend to rate the human-likeness of high-quality recordings of human speech as around or below 4.5 on a5 point scale; see for example the naturalness scores in the largeand careful evaluation in [?]. Complete human-likeness may thusin practice be achieved at a score below the maximum on the anygiven ratings scale. All that said, we believe that human-likenesscan and will be improve further in the future, especially with moreaccurate motion capture and more lifelike avatars to display motionon.

As for the path that the gesture generation will take towardsachieving new heights in human-likeness, we can look to history,and to other fields. Data-driven generative modeling like [GBK∗ 19,FNM20, ALIM20, KJvW∗ 20, YKJ∗ 19] took over as the state ofthe art in co-speech gesture generation with the advent of pub-licly available motion capture datasets suitable for training deep-learning architectures. Since then, a variety of deep generativeapproaches have been applied (see Table 2), and human-likenesskeeps improving [KJY∗ 21, RGP21, YWK∗ 22]. There is no doubtinteresting work to come in applying recent diffusion models [SD-WMG15, SE19, HJA20], already considered for general motionsynthesis [TRG∗ 22], to gesture generation. While generated ges-tures from data-driven machine learning models are convincing,a lack of large scale gesture datasets currently limit the human-likeness of these approaches. Hence, in the short term, we may ex-pect hybrid systems such as GestureMaster [ZBC22,AGL∗ 22] to bethe leaders in human-like gesture generation. Specifically, these aresystems where machine-learning decides which general propertiesare needed of the gestures, but the actual gesture motion is primar-ily realized by assembling pre-recorded motion clips and frames,like in motion graphs [LCR∗ 02,KGP02,AF02,TLP07] and motionmatching [Cla16]. In the long-term, however, purely deep learningmodels are likely to take over. This would match the trajectory fol-lowed by text-to-speech synthesis, where hybrid systems once gavethe best perceptual quality [Kin14], but pure deep-learning-basedapproaches trained on very large speech databases have recentlytaken the crown [TQSL21].

### 6.4. Multimodal Grounding

Visually human-like gesticulation is not the only goal of gesturegeneration. As discussed in the introduction to this article, a keygoal with generating co-speech gestures is to facilitate communi-cation, in much the same way as gestures enrich human commu-nication. This requires gestures that not only exhibit human-likemovement on the surface but also are appropriately grounded inthe context of the interaction, so that they can contribute to it. Inmore engineering-oriented terms, systems must take many relevantmodalities as input, and make use of this information in an adequateway, to obtain synthetic gestures that can fulfill the same commu-nicative roles as human gesticulation does. It can be difficult tocapture this information both in training data and at synthesis time,as well as to make meaningful use of it in the gesture generation.Grounding information can take many forms. Consequently, thissection discusses challenges in grounding gesture-generation in avariety of relevant multimodal aspects (system inputs), beginningwith aspects internal to the speaking agent, and then discussinggrounding in other parties in the conversation as well as in the sur-rounding space. More specifically, we cover grounding in

1. temporal information (Section 6.4.1);2. semantic content (Section 6.4.2);3. speaker identity, personality, emotion, and style (Section 6.4.3);4. interlocutor behavior (Section 6.4.4); and5. spatial information (Section 6.4.5).

We also discuss some derived challenges posed by the often weakcorrelation between grounding information and the gesture motion(Section 6.4.6), and how gestures may be grounded in the creativeintent of a system designer (Section 6.4.7).

#### 6.4.1. Temporal Grounding

Gestures are temporal, which is a result of their correlation witha heavily temporal acoustic modality, along with the fact thatthey might depict occurrences or trace out paths or shapes overtime. The rhythmic nature of the gestures (i.e. beat gestures) incontext of acoustic prosody has been studied heavily since theera of rule based gesture synthesis [CMM99, MXL∗ 13]. Fastforward to approaches with data-driven synthesis, some explic-itly rely on extracted prosodic features [FNM21], while others[GBK∗ 19, ALNM20] learn implicit embeddings from acousticswhich prosody is one of the key components. It seems clear thatgesture production must be grounded in the rhythm of audio data,and appropriate beat gestures will be challenging to achieve fromtext transcriptions alone, without timing information [KNN∗ 22].Alternatively, both audio and gesture must be synthesized to havecomparable rhythmic structure.

#### 6.4.2. Semantic Grounding

Beyond the rhythmic nature of gestures, there is often a seman-tic meaning associated with the performed gesture. The small sizeof gesture-generation databases, and the complicated relationshipand weak correlation between speech semantics and gesture form(see Section 6.4.6), mean that it is unrealistic to expect systemsto learn to generate semantically appropriate gestures driven byspeech acoustics alone. Text, on the other hand, is a compact wayto represent much of the semantic content behind co-speech ges-tures, and has been heavily studied since the era of rule-based ges-ture synthesis [CVB01] as well as in data-driven synthesis [SB19,LAM21, ALIM20, KJvW∗ 20, YCL∗ 20, ZYL∗ 22, LFZ∗ 22]. Cur-rent data-driven approaches typically attempt to gain semanticawareness by relying on deep-learning based language modelstrained on large amounts of text, such as [MCCD13, DCLT18].Recent large language models based on large amounts of text[BMR∗ 20] have indeed been capable of generating text with sur-prisingly coherent semantics, suggesting that they can capture lex-ical meaning to a significant extent. While the inclusion of texthas improved human perception of automatically generated ges-tures [KJvW∗ 20, ALIM20, YCL∗ 20, AGL∗ 22], it is still not trivialto measure the semantic content of gestures (see the discussion inSection 6.1). Hence, it is unclear how much (if any) of the im-proved human perception can be attributed to the semantic aware-ness created due to the use of language models, nor how much ofthe bottlenecks that exist may be removed with continuing progressin neural language models. More broadly, there is a need for gesturesynthesis models to perform better with regard to semantics. Ges-ture is most powerful when it conveys information, and doing thiseffectively has been a challenge for most deep learning systems; cf.Figure 4.

#### 6.4.3. Identity, Style, Emotion, and Personality

Co-speech gestures are idiosyncratic. The manifold of gesturesperformed by a speaker are not just a function of the contentof the speech, but are also dependent on the identity, emotionalstate and the context of the speaker. Generating personalized ges-tures based on speaker identity became possible with the influxof large scale multi-speaker datasets [YCL∗ 20, ALNM20]. Sev-eral GENEA Challenge 2022 [YWK∗ 22] systems also make use ofspeaker identity. A deeper analysis of the impact of speaker iden-tity input [KNN∗ 22] shows that different speakers have differentgesture-property prediction certainty, evoking even more interestin the idiosyncrasies of co-speech gestures. More recently, it wasalso shown that a short motion clip can be used for style control in“zero-shot style adaptation” [FGPO22, GFC22, GFH∗ ]. For manyapplications, it is desirable for the designer to be able to control thenature of the motion. This goes beyond replicating idiosyncraticmotion recorded of an individual to being able to specify novelcharacters. We are far from having ways to author a character withan imagined personality for a particular application.Apart from the speaker identity, the emotional or affective stateof a speaker also impacts the gestures performed by them. Astriking example of this is the large range of expressive motionvariation with the same lexical message explored in the Mime-bot data [AONB17]. Building emotionally aware embodied agentsis a common research direction [CBFV16, SZGK18]. More re-cently, data-driven models have been explored where affective cueswere learned using a dedicated encoder in an adversarial setup[BCRM21] to imitate these patterns of affective behavior. It is im-portant to be able to drive these emotions in a way that is consis-tent with a character’s personality and to be able to shift mood andemotion over time. One way forward might be to leverage findingsfrom the literature of gesture and motion perception, which hasidentified many useful properties of gesture motion that correlatewith the perception of personality [Lip98, KG10, SN17] and emo-tion [NLK∗ 13,CN19]. By changing these properties in synthesizedgestures, we may exert some control over the perceived speakerpersonality and emotion [AHKB20, HES∗ 22]. Again, speech syn-thesis provides an analogy, where it was recently shown that simpleand easy additions of filler words and pausing can meaningfully andreliably be used to alter listeners’ perception of speaker certainty[KLSG22].

#### 6.4.4. Interlocutor-Aware Gestures

While non-verbal behavior is impacted by internal state of thespeaker, the external context also guides the types of gestures aspeaker might perform. In a dyadic conversation, the model must beaware of the behavior of the interlocutor while generating the rel-evant gestures [AMMS19, JKHB20, NC22, YYH20]. This includesmodeling appropriate listener behavior as well as speaker behav-ior. Characters must modify their behavior to react to the content,mood and timing of interlocutors. Characters must be able to besurprised, angered, pleased, etc. based on what their interlocutormay say. Given the increasing availability of dyadic datasets withmotion capture for both conversational parties, we expect to seemore research in this direction in the next few years.

#### 6.4.5. Spatially Aware Gestures

Even more generally, the context could also include spatial un-derstanding of the environment. For example, the correctness ofdeictic gestures relies on the information about objects and direc-tions in a scene. To carry communicative value, most of these ges-tures will therefore require access to visual and/or spatial informa-tion beyond what may be contained in the speech – think abouta phrase such as “You need to go that way”, which completelylacks information about which direction the system should point.People also use spatial configurations in complex ways while ges-turing, for example, placing ideas in a referential space in frontof them and then referring to ideas by referring to the space theyhave been located in. While studies that involve external contextsare quite common for downstream tasks like navigation [SKM∗ 19],non-verbal behavior generation in multiple external contexts is upand coming [DWAB22, KNN∗ 22] which makes it a promising re-search direction, if relevant data can be obtained.

#### 6.4.6. Weak Correlations with Grounding Information

Let’s imagine that we have access to all the variables discussedthus far that impact the dynamics of co-speech gestures, such asacoustics, text, speaker identity, emotional state, and external con-texts. Further imagine that we are able to gather large-scale datasetswith all these variables, which is unlikely to ever happen due tothe combinatorical explosion of possible combinations of differ-ent factors. Would having this rich input information and broaddata coverage be sufficient to confidently predict the specifc co-speech gestures that a given speaker will perform? The best we canlikely say is “Maybe!” While large scale datasets may enable usto minimize our epistemic uncertainty about gesticulation, it is un-clear how significant the stochasticity is, i.e. aleatoric uncertainty,of these gestures will be. The situation is analogous to the prob-lem of prosody in text-to-speech, where there can be many possi-ble acoustic realizations and intonation contours for the same lexi-cal input [WWK15, LTHY17, WSZ∗ 18]. Significant variation per-sists even when a speaker is asked to read the same text severaltimes under exactly the same circumstances [HMS∗ 14]. To han-dle ambiguity in gesture realization, it is compelling to considerprobabilistic models, since they can “hallucinate” the missing in-formation and stochastic components of non-verbal behavior, as away to resolve the one-to-many problem for motion and gesturegeneration [HAB20, AHKB20].

#### 6.4.7. Grounding Gestures in Creative Intent

Gesture authoring enables an animator or system creator to de-sign and edit motion, e.g. making a character appear less nervousor stressed, thus grounding the animation within the designer’s cre-ative intent. Typically, animation design intent is captured throughkey-framing or motion capture. However, these approaches are dif-ficult to scale for nonverbal behavior because the former requiresspecialized animation skills, while the latter requires expensivecamera setups and laborious post-processing. Automatic gesturegeneration approaches in part solve the scalability issue by the abil-ity to generate abundant motion data, but they struggle with high-level control. For instance, attempts at handling control either bakein mechanistic, low-level control signals like wrist height, wrist ve-locity, and radial extent [AHKB20], or they generate gestures thatdeviate from the intended control specifications [HES∗ 22]. More-over, in multi-speaker scenarios, they are unable to capture the vari-ability of different speakers’ gesticulation, and cannot distinguishbetween gesture types used in a certain scenario (e.g. deictic ges-tures for a lecturer in front of display) from gesture style differencesbetween speakers [ALNM20]. Yoon et al. [YPJ∗ 21] recently pro-posed an innovative approach to this challenge: an authoring toolkitthat balances gesture quality and authoring effort. The toolkit com-bines automatic gesture generation using a GAN-based generativemodel [YCL∗ 20] and manual controls. The generative model firstproduces a gesture sequence from speech input, and animator caninteractively edit the motion through low-level pose control andcoarse-level style parameters. We think similar gesture authoringapproaches that maximize design intent and gesture quality, whileminimizing authoring effort will be important for grounding non-verbal behavior within the animator’s creative intent.

### 6.5. Multimodal Synthesis

Human communicative behavior is not only grounded in multiplemodalities and information streams, but is also expressed throughmultiple modalities. A complete virtual agent agent will need tolisten, observe, decide, speak, and move. On the generation side,verbal behavior generation is considered separate from non-verbalbehavior, and the generation of non-verbal behavior is in turn typi-cally broken into several smaller sub-problems treated in isolation.Head motion might be treated separately from lip motion, facial ex-pression, and gaze; finger motion might be treated separately fromarm motion; and lower-body motion might be separated from themotion of the upper body. A long-term goal would be to bringthese sub-problems together, to create more coherent synthetic be-havior with a wider range of possible expressions, and eventuallyunify the synthesis of these expressions with verbal behavior gen-eration. Recent work has explored learning full-body gesture mo-tion (including the head and the lower body), e.g. [AHKB20] andthe submissions to the full-body tier of the GENEA Challenge2022 [YWK∗ 22].

Another line of work has considered training verbal (text-to-speech) and non-verbal (speech-to-gesture) synthesis systems onthe same data [ASH∗ 20] and, subsequently, merging them into onesingle network that generates both speech audio and gesture motion[WAG∗ 21]. Given the strides that have been made in generatingconvincing speech audio from text [TQSL21], adapting successfultext-to-speech methods to simultaneously generate both acousticsand joint rotations, as was done in [WAG∗ 21], seems like a com-pelling direction for future work. This not only brings advantages interms of modeling efficiency (the gesture-generation systems willpossess information about, e.g. prosodic prominence without hav-ing to learn to extract that information from speech audio), but alsomore closely resembles models of human communication such asthe growth-point hypothesis [McN92a], and could enable gesturesthat not only complement but, as in Kendon’s continuum (see Fig-ure 2), replace or augment speech with novel information. Thismay require even deeper representations of communicative intent,as approaches that generate gesture based on text and/or audio arerestricted to redundant gestures, but gesture that is non-redundantwith the spoken audio is a key part of human behavior.

## 7. Broader Impact

High quality gesture synthesis can advance a range of applicationsby allowing computational systems to leverage nonverbal commu-nication. This can allow more natural and fluid communication ofboth functional and affective information, which will prove use-ful in a range of assistive applications, employing both agentsand robots. These include tutors, rehabilitation trainers, relationalagents for health and eldercare, and personal assistants. They canalso support richly interactive entertainment experiences in whichyou can have meaningful interactions with virtual characters.

The development of the technology also raises potential ethicalissues which must be given careful consideration. Some of the is-sues are common to many deep learning approaches that involvehuman data. For instance, what kind of bias is in the data that isused? Does it represent the full range of human nonverbal behavior,or only specific language groups, ethnicities and social strata? Willpeople using these models take care to match the input data withthe desired output representation or will the data be mismatched,using the wrong gender, ethnicity, age, etc. on synthesized charac-ters? What are the ownership rights associated with data that maybe scraped from a web source? Do you own your gesture style?How can consent be obtained for online data?

The technology could also make it easier to generate deepfakes,i.e., synthetic media that mimics the likeness of real people, espe-cially of politicians and other public figures that have a lot of videodata online. Prominent examples include photorealistic lip motionfrom audio [SSKS17], real time facial expression re-enactment[TZS∗ 16] and talking-head video synthesis [WML21]. The tech-nology can be adapted to create synthetic nonverbal motion for ne-farious purposes such as political propaganda, financial fraud andfake news. Moreover, a more unique consideration for nonverbalbehavior results from people’s tendency to entrain to their inter-locutors. If they entrain to synthetic models they may interact with,does this have any impact on their own behavior? It is important forboth researchers and developers of this technology to devise waysto mitigate these risks.

## 8. Conclusion

This paper summarizes the history of gesture generation, from earlywork on rule-based systems to the explosion of recent work us-ing deep learning approaches. Deep learning approaches have em-ployed a range of input, including text, audio and various con-trol signals, and used a wide set of architectures. Most systemshave focused on monologue generation, but work is beginning toexplore dialog and richer notions of context. Despite substantialprogress, the field is still young and there are very significant chal-lenges to solve. These include better datasets, improved subjectiveand objective evaluation practices, higher quality motion, produc-ing more meaningful gestures, adequately addressing the stochas-ticity of gesture, providing adequate control over the output andmatching the rich set of grounding that supports human gesture,from multi-person interaction to adequately representing the spa-tial context of the conversation. There is much exciting work tocome.
