\chapter{Kết luận}
\label{Chapter5}

\section{Kết quả đạt được}

%Trong luận văn này, mô hình sinh cử chỉ của chúng tôi đã có kết quả sinh cử rất giống người, và hệ thống không chỉ có khả năng sinh cử chỉ đồng bộ với cảm xúc và nội dung của âm thanh đầu vào mà còn có thể suy luận dữ liệu không có trong tập huấn luyện. Ngoài ra, chúng tôi cũng đóng góp mô hình pretrain trên Huggingface (\hyperlink{https://huggingface.co/openhuman/openhuman}{huggingface.co/openhuman/openhuman}) và toàn bộ mã nguồn chương trình OHGesture ở github công khai \hyperlink{https://github.com/hmthanh/OHGesture}{hmthanh/OHGesture}.
%Chúng tôi cũng upload toàn bộ các video render của mô hình lên Youtube để có thể so sánh kết quả giữa các mô hình, cũng như kết quả minh hoạ của mô hình OHGesture và GroundTruth.
%
%Chúng tôi mở rộng dữ liệu đầu vào không chỉ bao gồm cử chỉ, âm thanh và nhãn cảm xúc. Mà còn sử dụng các công cụ chuyển văn bản thành giọng nói để transcribe âm thanh thành văn bản. Từ đó có thêm nguồn dữ liệu cho quá trình học khử nhiễu có điều kiện.
%
%Tuy nhiên, do hạn chế tài nguyên, các đánh giá hiện tại vẫn chủ yếu dựa trên các kết quả chủ quan và kết quả từ mô hình cơ sở, chưa hoàn toàn phản ánh chất lượng thực tế của hệ thống đề xuất.

Trong luận văn này, chúng tôi đã phát triển và thử nghiệm thành công mô hình sinh cử chỉ OHGesture, một hệ thống có khả năng tạo ra cử chỉ rất tự nhiên, mang lại cảm giác giống người. Điểm nổi bật của mô hình là khả năng đồng bộ chính xác giữa cử chỉ và cảm xúc, nội dung của âm thanh đầu vào, đồng thời có khả năng suy luận vượt ra khỏi các dữ liệu được huấn luyện ban đầu. Điều này có nghĩa là mô hình dựa trên Diffusion  không chỉ phụ thuộc vào các mẫu dữ liệu cử chỉ đã được học mà còn có thể có tính khái quát hoá cao, và có thể sinh cử chỉ cho các âm thanh và ngữ cảnh có xác xuất dữ liệu thấp.

%Để hỗ trợ cộng đồng nghiên cứu và mở rộng ứng dụng, chúng tôi đã cung cấp phiên bản pretrained của mô hình OHGesture trên nền tảng Huggingface, có thể truy cập tại địa chỉ \hyperlink{https://huggingface.co/openhuman/openhuman}{huggingface.co/openhuman/openhuman}. Bên cạnh đó, toàn bộ mã nguồn của mô hình cùng các cải tiến trong quá trình xử lý dữ liệu và sinh cử chỉ đã được công khai trên GitHub tại kho lưu trữ \hyperlink{https://github.com/hmthanh/OHGesture}{hmthanh/OHGesture}, cho phép các nhà nghiên cứu và nhà phát triển dễ dàng tải xuống, tùy chỉnh và thử nghiệm. Để hỗ trợ việc đánh giá và so sánh kết quả, chúng tôi cũng đã tải lên toàn bộ các video render của mô hình trên YouTube, giúp trực quan hóa và đối chiếu giữa kết quả cử chỉ sinh ra bởi OHGesture và cử chỉ từ dữ liệu thực tế (GroundTruth).

Một điểm đáng chú ý khác trong nghiên cứu này là việc mở rộng dữ liệu đầu vào. Chúng tôi không chỉ giới hạn ở cử chỉ, âm thanh và nhãn cảm xúc mà còn tích hợp thêm các công cụ chuyển đổi văn bản thành giọng nói để chuyển âm thanh thành văn bản. Nhờ vậy, mô hình có thêm nguồn thông tin để học cách khử nhiễu có điều kiện, giúp hệ thống hiểu rõ hơn ngữ cảnh và tạo ra cử chỉ phù hợp với từng nội dung cụ thể.

%Tuy nhiên, do hạn chế về tài nguyên, việc đánh giá chất lượng của mô hình hiện tại vẫn chủ yếu dựa trên các kết quả chủ quan và kết quả của các mô hình cơ sở. Dù cho các kết quả đánh giá này khá khả quan, chúng chưa thể hoàn toàn phản ánh chất lượng thực tế của hệ thống đề xuất trong mọi bối cảnh và điều kiện. Điều này là một hạn chế cần được cải thiện trong các nghiên cứu tiếp theo nhằm đạt được sự đánh giá chính xác và toàn diện hơn.



\section{Ưu và nhược điểm của mô hình}


Mô hình sinh cử chỉ OHGesture có nhiều ưu điểm đáng kể, góp phần quan trọng trong việc phát triển các hệ thống tương tác người-máy tự nhiên và linh hoạt hơn. Tuy nhiên, vẫn còn một số nhược điểm để cải thiện hiệu quả trong tương lai.

\textbf{Ưu điểm:}

\begin{itemize}
	\item Độ chân thực cao: Dựa vào kết quả sinh cử chỉ, có thể thấy mô hình OHGesture đã đạt được kết quả sinh cử chỉ có độ giống người cao. Các cử chỉ sinh ra phản ánh được sắc thái và nhịp điệu của âm thanh, giúp hệ thống có khả năng phản hồi đồng bộ với thông tin cảm xúc và nội dung của lời nói.
	
	\item Khả năng tổng quát tốt: Nhờ mô hình khử nhiễu có thể phủ được các điểm dữ liệu có các xác xuất thấp, mô hình có thể suy luận cử chỉ cho các tình huống và trạng thái cảm xúc chưa từng xuất hiện trong tập huấn luyện, cho thấy tiềm năng ứng dụng trong nhiều bối cảnh thực tế.
	
	\item Có khả năng kiểm soát được nhiều đặc trưng khác nhau: Mô hình diffusion có khả năng điều khiển được các cảm xúc khác nhau, có khả năng nội suy trạng thái giữa các cảm xúc khác nhau.
	
%	\item 
	
%	Dễ dàng triển khai và mở rộng: Việc công khai mã nguồn và mô hình pretrain trên Huggingface giúp cộng đồng dễ dàng tiếp cận, triển khai và mở rộng mô hình. Điều này tạo điều kiện cho các nhà nghiên cứu và nhà phát triển khác đóng góp, cải tiến mô hình một cách nhanh chóng và hiệu quả.
	
	\end{itemize}



\textbf{Nhược điểm:}

\begin{itemize}
	\item Mô hình hiện chưa có khả năng sampling theo thời gian thực (real time) và cần nhiều bước để ra kết quả cuối cùng.
	
	\item Thông tin đặc trưng theo $D=1141$ được xử lý như một tấm ảnh, không thể hiện đúng đặc trưng chuyển động.
	
	\item Phụ thuộc vào dữ liệu đầu vào chất lượng cao: Mô hình yêu cầu dữ liệu âm thanh đầu vào có chất lượng tốt và rõ ràng để đảm bảo độ chính xác của cử chỉ sinh ra. Khi âm thanh đầu vào bị nhiễu hoặc chứa nhiều biến thể cảm xúc khó phân biệt, độ chính xác của cử chỉ sinh ra có thể giảm sút.
	
%	\item 
%	Thiếu các tiêu chuẩn đánh giá khách quan: Hiện nay, các đánh giá về chất lượng cử chỉ sinh ra vẫn dựa nhiều vào ý kiến chủ quan của người dùng, khiến việc so sánh chính xác giữa các mô hình khác nhau gặp khó khăn. Việc thiếu các phương pháp đánh giá tự động, đáng tin cậy làm hạn chế khả năng cải tiến liên tục của mô hình.
	
%	\item Yêu cầu tài nguyên tính toán lớn: Mô hình cần tài nguyên tính toán cao, như GPU mạnh, để có thể huấn luyện và suy luận hiệu quả, điều này có thể là rào cản cho các ứng dụng thực tế trên thiết bị có cấu hình thấp hoặc trong môi trường tính toán giới hạn.
	
	
\end{itemize}


%\section{Hạn chế và hướng phát triển}
%
%Mặc dù hệ thống sinh cử chỉ của chúng tôi đã đạt được những kết quả khả quan, vẫn còn một số hạn chế cần khắc phục. Đầu tiên, chất lượng sinh cử chỉ có thể bị ảnh hưởng bởi tính phức tạp của âm thanh đầu vào, đặc biệt khi âm thanh chứa nhiều yếu tố gây nhiễu hoặc cảm xúc chưa được biểu đạt rõ ràng. Bên cạnh đó, việc đánh giá chất lượng mô hình vẫn chủ yếu dựa trên đánh giá chủ quan của con người, dẫn đến khả năng thiếu nhất quán và khó khăn trong việc so sánh chính xác với các mô hình hiện có.
%
%Trong tương lai, hướng phát triển tiếp theo sẽ tập trung vào việc tối ưu hóa mô hình để cải thiện độ chính xác và tính ổn định của cử chỉ sinh ra trong các tình huống phức tạp. Đồng thời, việc mở rộng dữ liệu huấn luyện để bao quát thêm nhiều trạng thái cảm xúc và bối cảnh khác nhau có thể giúp mô hình đáp ứng tốt hơn các tình huống đa dạng. Chúng tôi cũng đề xuất nghiên cứu thêm các phương pháp đánh giá tự động, nhằm giảm thiểu sự phụ thuộc vào các đánh giá chủ quan. Bên cạnh đó, tích hợp các công cụ trí tuệ nhân tạo khác, như mô hình hiểu ngữ cảnh và phân tích cảm xúc, sẽ là bước tiến quan trọng để nâng cao tính linh hoạt và độ chính xác của hệ thống trong việc hiểu và sinh động các cử chỉ đồng bộ với nội dung và cảm xúc của lời nói.

\section{Phương hướng phát triển và nghiên cứu trong tương lai}


Trong tương lai, có nhiều hướng phát triển tiềm năng để cải thiện và mở rộng mô hình sinh cử chỉ OHGesture, nhằm đáp ứng tốt hơn các yêu cầu thực tế và nâng cao tính ứng dụng của hệ thống. Một số hướng nghiên cứu và phát triển chính bao gồm:

\begin{itemize}
	\item Tối ưu hoá mô hình để có thể chay theo thời gian thực, hiện tại mô hình phải chia chuỗi âm thanh, và kết quả sinh phải được  import vào Unity mới có thể render. Trong tương lai, chúng tôi mong muốn có thể xây dựng các hệ thống với thời gian thực. để có thể tương tác và tăng tính ứng dụng của mô hình
	
	\item Tối ưu hóa quá trình sampling và giảm số bước lấy mẫu: Hiện tại, quá trình sinh cử chỉ đòi hỏi một số bước lấy mẫu (sampling steps) tương đối lớn, ảnh hưởng đến tốc độ và hiệu quả của hệ thống. Việc tối ưu hóa để giảm số bước sampling mà không làm suy giảm chất lượng cử chỉ sinh ra sẽ giúp mô hình đáp ứng nhanh hơn và phù hợp cho các ứng dụng thời gian thực.
	
	\item Tích hợp và thử nghiệm các kỹ thuật embedding mới: Sử dụng các phương pháp embedding mới, đa dạng hóa thông tin đầu vào có thể giúp mô hình hiểu và phản ánh tốt hơn ngữ cảnh và cảm xúc của giọng nói. Đây cũng là một hướng phát triển để nâng cao khả năng của hệ thống trong việc sinh cử chỉ phù hợp với ngữ nghĩa của các ngôn ngữ khác nhau.
	
	\item Mở rộng sang đa ngôn ngữ và đa văn hóa: Hiện tại, mô hình chủ yếu hoạt động với các dữ liệu âm thanh tiếng Anh. Nghiên cứu mở rộng mô hình để sinh cử chỉ cho các ngôn ngữ và nền văn hóa khác nhau sẽ là một bước tiến quan trọng, giúp hệ thống trở nên đa dạng và ứng dụng rộng rãi hơn.
	
	\item Kết hợp với mô hình DeepPhase \cite{starke2022deepphase} để đạt hiệu quả sinh cử chỉ thời gian thực: Mục tiêu của chúng tôi là tích hợp OHGesture với mô hình DeepPhase để phát triển hệ thống có khả năng phản hồi cử chỉ trong thời gian thực, ứng dụng trong các tình huống giao tiếp tự nhiên như hội thoại người-máy và các hệ thống điều khiển bằng giọng nói. Với mục tiêu là sẽ học các đặc trưng về pha của các chuyển động để có thể trích xuất được các đặc trưng chuyển động, thay vì sử dụng đặc trưng như một tấm ảnh của mô hình.
	
%	\item Trong tương lai, 
	
	\item Cải thiện đánh giá khách quan bằng các độ đo tự động: Để giảm sự phụ thuộc vào các đánh giá chủ quan, cần phát triển và tích hợp các phương pháp đánh giá tự động đáng tin cậy, cho phép mô hình có thể tự đánh giá và điều chỉnh theo các chỉ số khách quan.
	\end{itemize}



\section{Đóng góp của luận văn}

Trong luận văn này, chúng tôi đã phát triển hệ thống sinh cử chỉ OHGesture, với các đóng góp quan trọng như sau:

\begin{itemize}
	\item Phát triển mô hình sinh cử chỉ dựa trên Diffusion: Hệ thống OHGesture được thiết kế để tạo ra các cử chỉ đồng bộ với âm thanh đầu vào và phản ánh đúng cảm xúc. Mô hình này còn có khả năng tổng quát hoá, cho phép sinh cử chỉ ngay cả với các mẫu âm thanh ngoài dữ liệu huấn luyện, mang lại độ chân thực cao.
	
	\item Công khai mã nguồn và mô hình trên các nền tảng mở: Để thúc đẩy ứng dụng và cải tiến từ cộng đồng, chúng tôi đã cung cấp mã nguồn trên GitHub, chúng tôi mở rộng mã nguồn, và được công khai ở \hyperlink{https://github.com/hmthanh/OHGesture}{hmthanh/OHGesture} và một phiên bản pretrained trên Huggingface \hyperlink{https://huggingface.co/openhuman/openhuman}{huggingface.co/openhuman/openhuman}, giúp các nhà nghiên cứu khác dễ dàng tiếp cận, tái hiện và mở rộng hệ thống.
	
	\item Tích hợp thêm văn bản và âm thanh chuyển ngữ trong quá trình sinh cử chỉ: Với tập dữ liệu ZeroEGGS chỉ bao gồm âm thanh, cử chỉ và nhãn cảm xúc, chúng tôi sử dụng các API của Azure và Google để transcribe các tệp âm thanh thành văn bản. Giúp mở rộng dữ liệu đầu vào của mô hình sinh cử chỉ thêm đặc trưng văn bản, giúp hệ thống có thêm ngữ cảnh để sinh ra các cử chỉ phù hợp hơn với nội dung cụ thể.
	
	\item Đóng góp vào việc xây dựng hệ thống đánh giá chuẩn hóa: Chúng tôi đang phát triển một hệ thống xếp hạng trực tuyến \hyperlink{https://genea-workshop.github.io/leaderboard/}{GENEA Leaderboard} \cite{nagy2024towards} cho các mô hình sinh cử chỉ. Chúng tôi đã thu thập và xử lý các dữ liệu cử chỉ từ nhiều nguồn ngôn ngữ và tập dữ liệu khác nhau, tạo thành một tập dữ liệu chuẩn hóa duy nhất. Xây dựng một bản xếp hạng để so sánh nhiều mô hình khác nhau. Và sử dụng người để đánh giá các mô hình sinh, giúp đánh giá chính xác kết quả sinh của cử chỉ so với các độ đo trước đây, vốn không thể đo được sự phức tạp và đa dạng trong chuyển động của cử chỉ tương ứng với âm thanh. Điều này giúp tạo nên một nền tảng dữ liệu thống nhất, hỗ trợ việc đánh giá đồng nhất giữa các mô hình. Từ đó thúc đẩy tính thống nhất trong lĩnh vực sinh cử chỉ trong cộng đồng nghiên cứu.
	
	\item Xây dựng công cụ trực quan hoá bằng Unity: Các hệ thống trực quan hoá cử chỉ hiện nay đề dự trên Blender, và không đạt kết quả hiển thị tốt khi sinh cử chỉ, bằng việc kết thừa mã nguồn từ mô hình \cite{starke2022deepphase}, chúng tôi đã tạo ra một hệ thống trực quan hoá quá sinh sinh cử chỉ và so sánh giữa các mô hình. 
	
	\item Xây dựng hướng phát triển trong tương lai: Dựa trên nền tảng của mô hình diffusion và hiểu rõ bản chất của quá trình sinh cử chỉ, chúng tôi có thể kết hợp các mô hình sinh cử chỉ dựa trên pha với các thuật toán xử lý và trích xuất thông tin, nhằm tối ưu hóa chất lượng và tính phù hợp của các cử chỉ được sinh ra. Hướng phát triển này mở ra triển vọng cho các cải tiến sâu rộng trong việc tương tác giữa cử chỉ và các yếu tố ngữ cảnh phức tạp như biểu cảm khuôn mặt, ngữ điệu và động lực cảm xúc, tạo nền tảng cho những bước tiến trong các ứng dụng giao tiếp người - máy và các lĩnh vực liên quan khác.
\end{itemize}








\section{Lời kết}
%\ref{fig:SampleAnimation}
%Qua kết quả sinh cử chỉ thực tế sau khi render, có thể thấy mô hình của chúng tôi kế thừa từ mô hình DiffuseStyleGesture đạt kết quả tốt khi kết xuất, không chỉ có thể sinh ra cử chỉ chân thật từ dữ liệu GroundTruth mà còn có thể suy luận ra những cử chỉ không hề có trong tập dữ liệu huấn luyện như âm thanh của Steven Jobs nói. Điều này cũng chứng tỏ rằng mô hình Diffusion là mô hình có thể đạt kết quả tốt để có thể phủ được các kết quả sinh mà ở đó xác xuất dữ liệu thấp. Ngoài đóng góp về việc tích hợp thêm văn bản vào quá trình sinh cử chỉ, mã nguồn chỉnh sửa được công khai trên Github, chúng tôi cũng đóng góp về quá trình kết xuất và quá trình xử lý dữ liệu trên Unity. Đây là nền tảng để tiếp tục nghiên cứu và cải thiện mô hình OHGesture hiện tại.

%Việc học cử chỉ tương ứng với giọng nói (speech-drive gesture generation) không chỉ đạt kết quả tốt với dữ liệu gốc mà cử chỉ còn có thể sinh ra cử chỉ đạt kết quả tốt ở những âm thanh không có trong dữ liệu như 


%Trong tương lai, vẫn còn nhiều bước có thể cải tiến, như giảm số bước sampling, tối ưu các tham số và sử dụng các embedding mới, cũng như sinh cử chỉ cho các văn bản của ngôn ngữ khác. Mục tiêu của chúng tôi là tích hợp với mô hình \hyperlink{https://deepgesture.github.io/}{DeepGesture} để có thể sinh cử chỉ theo thời gian thực.




Qua quá trình thử nghiệm và phân tích kết quả sinh cử chỉ thực tế, mô hình OHGesture của chúng tôi đã kế thừa và phát triển từ mô hình DiffuseStyleGesture, có khả năng sinh cử chỉ chân thực, không chỉ trên các mẫu dữ liệu trong tập huấn luyện mà còn mở rộng được với những âm thanh không có trong dữ liệu huấn luyện, ví dụ như giọng nói của Steven Jobs. Điều này minh chứng cho tiềm năng của mô hình Diffusion trong việc sinh cử chỉ cho các dữ liệu hiếm, nơi xác suất dữ liệu thấp.

Bên cạnh đó, chúng tôi đã đóng góp các mã nguồn chỉnh sửa trên Github, bao gồm quá trình kết xuất và xử lý dữ liệu trên nền tảng Unity, tạo nền tảng thuận lợi cho các nghiên cứu và cải tiến tiếp theo đối với mô hình OHGesture. Việc kết hợp thêm văn bản vào quá trình sinh cử chỉ cũng là một bước đột phá, mở ra nhiều hướng phát triển ứng dụng trong các lĩnh vực yêu cầu tương tác tự nhiên và hiệu quả hơn.

%Qua quá trình thực nghiệm, chúng tôi có thể chứng mô hình \textbf{OHGesture} đạt được kết quả sau:

%1). Mô hình đạt kết quả tốt trong việc sinh cử chỉ dựa trên âm thanh, tạo ra các cử chỉ chất lượng cao, giống con người (human-likeness).
%
%2) Dựa trên phương pháp Classifier-Free Guidance, chúng tôi có thể sử dụng mô hình diffusion có điều kiện, với điều kiện bao gồm cảm xúc và cử chỉ khởi tạo. Mô hình đã có thể nội suy ra được những cảm xúc, hoặc khoảng cách ở giữa hai cảm xúc. 
%
%3) Dựa vào cơ chế self-attention và cross attention, chúng tôi có thể sinh ra cử chỉ tương ứng với giọng nói, có khả năng liên với bối cảnh.

%Mô hình của chúng tôi tổng hợp các cử chỉ sao cho chúng khớp với nhịp âm thanh và ngữ nghĩa văn bản dựa trên các cơ chế tập trung chéo và tập trung tự.

%3) Sử dụng phương pháp huấn luyện hướng dẫn không cần bộ phân loại, chúng ta có thể kiểm soát các điều kiện cụ thể, tức là cảm xúc và cử chỉ khởi tạo, và thực hiện nội suy hoặc mở rộng để đạt được một mức kiểm soát cao đối với các cử chỉ được tạo ra.

%4) Kết quả thực nghiệm cho thấy, mô hình của chúng tôi có thể sinh cử chỉ đạt kết quả tốt ngay cả với trường hợp âm thanh không nằm trong tập huấn luyện.
%
%Trong các độ đo về human-likeness, emotion-likeness và speech appropriateness mô hình của chúng tôi vượt trội so với các phương pháp hiện có trong nhiệm vụ tạo ra cử chỉ đồng thời dựa trên âm thanh và thể hiện khả năng thao tác phong cách cao hơn.
%Trong hướng nghiên cứu tương lai, chúng tôi dự định sẽ sử dụng các thuật toán Fourier để tách chuỗi chuyển động của cử chỉ để có thể học được pha trong các chuyển động cử chỉ.
% Còn nhiều không gian để cải thiện trong nghiên cứu này, ví dụ, giải quyết vấn đề nhiều bước lấy mẫu và tiêu thụ thời gian lâu của các phương pháp diffusion để sử dụng trong các hệ thống thời gian thực là hướng nghiên cứu của chúng tôi trong tương lai.

% Trong phần này chúng tôi đưa ra các kết quả đạt được của mô hình chúng tôi, chúng tôi cố gắng tìm hiểu các đặc trưng của các bộ dữ liệu tương ứng để cố gắng lý giải thích tại sao mô hình của chúng tôi hoặc các công trình khác có được kết quả tốt trên tập dữ liệu tương ứng đó. Những kết quả của hai đề xuất của chúng tôi cũng như các dịnh hướng nghiên cứu của chúng tôi trong tương lai.

% Mặc dù kết quả chúng tôi cho thấy phương pháp của chúng tôi có hiệu suất tương đương với các mô hình học sâu hiện đại (state-of-art) và có ưu thế vượt trội trong thời gian đào tạo khoảng 17 phút so với thời gian hàng giờ của phương pháp học sâu khác nhưng không phải là các mô hình học sâu này không đáng nghiên cứu. Chúng tôi cũng nhận thấy đối với những tập dữ liệu khó như FB15-237 hay WN18RR phương pháp của chúng tôi thường cho kết quả không tốt do các quan hệ tương tự hay nghịch đảo không không xuất hiện trong ví dụ đào tạo nên chúng tôi khó tạo ra các luật đủ tốt để có thể khái quát hóa trên toàn bộ đồ thị đẫn đến các kết quả không tốt. 
% Ngược lại đối với các phương pháp dựa trên học sâu lại có ưu thế rất lớn trong các tập dữ liệu này do có thể dễ dàng tính toán độ gần của các luật mới cần đánh giá so với các luật đã học từ đó có một kết quả khá tốt. Do đó chúng tôi cũng sẽ tiếp tục nghiên cứu các phương pháp học sâu và sẽ dùng phương pháp này làm đường cơ sở (base line) để so sánh với các nghiên cứu của chúng tôi trong tương lai. Một điểm yếu nữa của mô hình đựa trên luật của chúng tôi là mặc dù thời gian học là vượt trội nhưng thời gian để tính toán đưa ra đự đoán khá lâu do phải duyệt qua tất cả các luật được sinh ra mới có thể đưa ra dự đoán. Không giống như các phương pháp nhúng đồ thị khác thao tác này có thể dễ dàng tính toán.

% Đối với hai thuật toán mở rộng của chúng tôi trong việc thêm tri thức mới vào đồ thị chúng tôi nhận thấy rằng là vượt trội hoàn toàn so với các phương pháp học sâu. Ở các phương pháp học sâu điều này đường như chưa được ai chú trọng nghiên cứu mặc dù thời gian đào tạo một mô hình là tương đối mất thời gian. Khi có tri thức mới hầu hết các mô hình phải đào tạo lại toàn bộ điều này khá lãng phí. Chúng tôi cũng xem đây là mục tiêu tiếp theo cho chúng tôi khi nghiên cứu các mô hình học sâu trong tương lai. Gần đây nhánh học tăng cường (reinforcement learning) khá phát triển và nhóm tác giả Meilicke, Christian and Chekol \cite{meilicke2020reinforced} gần đây cũng đã có 1 nghiên cứu để tối ưu hóa lại phương pháp AnyBURL này. Chúng tôi cũng có ý định nghiên cứu về hướng này và cố gắng báo cáo lại trong một tương lai gần.
