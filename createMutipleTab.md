[14] https://doi.org/10.18653/v1/N19-1423
 Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT:Pre-training of deep bidirectional transformers for language understanding. InProceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL '18). ACL. 
 
[15] https://doi.org/10.1002/cav.2016
 Ylva Ferstl, Michael Neff, and Rachel McDonnell. 2021. ExpressGesture: Ex-pressive gesture generation from speech through database matching. Comput.Animat. Virt. W. 32, 3-4 (2021), e2016. 
 
[16] Saeed Ghorbani, Ylva Ferstl, and Marc-André Carbonneau. 2022. Exemplar-basedstylized gesture generation from speech: An entry to the GENEA Challenge 2022.In Proceedings of the ACM International Conference on Multimodal Interaction(ICMI '22 ACM.

[17] https://doi.org/10.21437/ Interspeech.2019-1783
 Avashna Govender, Anita E. Wagner, and Simon King. 2019. Using pupil dilationto measure cognitive load when listening to text-to-speech in quiet and in noise.In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech '19, Vol. 20). ISCA, 1551-1555. 
 
[18] https://books.google.be/books?id=y3o0DgAAQBAJ
 Gerald J. Hahn and William Q. Meeker. 1991. Statistical Intervals: A Guide forPractitioners. Vol. 92. John Wiley & Sons. 
 
[19] https://doi.org/10.1155/2022/1828293
 Zhiyuan He. 2022. Automatic quality assessment of speech-driven synthesizedgestures. Int. J. Comput. Games. Tech. 2022 (2022). 
 
[20] https://doi.org/10.3758/s13423-017-1363-
 Judith Holler, Kobin H. Kendrick, and Stephen C. Levinson. 2018. Processing lan-guage in face-to-face conversation: Questions with gestures get faster responses.Psychon. B. Rev. 25, 5 (2018), 1900-1908. 
 
[21] https://www.jstor.org/stable/4615733[22] International Telecommunication Union, Telecommunication StandardisationSector. 1996. Methods for subjective determination of transmission quality. Recom-mendation ITU-T P.800. https://www.itu.int/rec/T-REC-P.800-199608-I
 Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scand.J. Stat. 6, 2 (1979), 65-70. 
 
[23] https://doi.org/10.1145/3383652.3423911
 Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, and Jonas Beskow. 2020.Let's face it: Probabilistic multi-modal interlocutor-aware generation of facialgestures in dyadic settings. In Proceedings of the ACM International Conference on Intelligent Virtual Agents (IVA '20). ACM, Article 31, 8 pages. 
 
[24] https://doi.org/10.1145/3383652.3423860
 Patrik Jonell, Taras Kucherenko, Ilaria Torre, and Jonas Beskow. 2020. Canwe trust online crowdworkers? Comparing online and offline participants ina preference test of virtual agents. In Proceedings of the ACM International Conference on Intelligent Virtual Agents (IVA '20). ACM, Article 30, 8 pages. 
 
[25] https://doi.org/10.1145/3462244.3479957
 Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, and Gustav EjeHenter. 2021. HEMVIP: Human evaluation of multiple videos in parallel. InProceedings of the ACM International Conference on Multimodal Interaction (ICMI '21). ACM, 707-711. 
 
[26] Naoshi Kaneko, Yuna Mitsubayashi, and Geng Mu. 2022. TransGesture: Autore-gressive gesture generation with RNN-transducer. In Proceedings of the ACMInternational Conference on Multimodal Interaction (ICMI '22 ACM.

[27] https://doi.org/10.3989/loquens.2014.006[28] Vladislav Korzun, Anna Beloborodva, and Arkady Ilin. 2022. ReCell: replicatingrecurrent cell for auto-regressive pose generation. In Companion publicationof the 2021 ACM International Conference on Multimodal Interaction (ICMI '22 ACM.
 Simon King. 2014. Measuring a decade of progress in text-to-speech. Loquens 1,1 (2014), e006. 
 
[29] Vladislav Korzun, Ilya Dimov, and Andrey Zharkov. 2021. Audio and text-drivenapproach for conversational gestures generation. In Proceedings of Computational Linguistics and Intellectual Technologies (DIALOGUE '21). http://www.dialog- 21.ru/media/5526/korzunvaplusdimovinpluszharkovaa031.pdf

[30] https://doi.org/10.1145/3308532.3329472
 Taras Kucherenko, Dai Hasegawa, Gustav Eje Henter, Naoshi Kaneko, and Hedvig Kjellström. 2019. Analyzing input and output representations for speech-driven gesture generation. In Proceedings of the ACM International Conference on Intel-ligent Virtual Agents (IVA '19). ACM, 97-104. 
 
[31] https://doi.org/10.1145/3382507.3418815
 Taras Kucherenko, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter, SimonAlexanderson, Iolanda Leite, and Hedvig Kjellström. 2020. Gesticulator: A frame-work for semantically-aware speech-driven gesture generation. In Proceedings of the ACM International Conference on Multimodal Interaction (ICMI '20). ACM, 242-250. 
 
[32] https://doi.org/10.1145/3397481.3450692
 Taras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav EjeHenter. 2021. A large, crowdsourced evaluation of gesture generation systemson common data: The GENEA Challenge 2020. In Proceedings of the ACM Annual Conference on Intelligent User Interfaces (IUI '21). ACM, 11-21. 
 
[33] https://doi.org/10.1145/3472306.3478333
 Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael Neff, Hedvig Kjellström,and Gustav Eje Henter. 2021. Speech2Properties2Gestures: Gesture-propertyprediction as a tool for generating representational gestures from speech. In Proceedings of the ACM International Conference on Intelligent Virtual Agents (IVA'21). ACM, 145-147. 
 
[34] https://doi.org/10.5555/3535850.3535937
 Taras Kucherenko, Rajmund Nagy, Michael Neff, Hedvig Kjellström, and Gus-tav Eje Henter. 2022. Multimodal analysis of the predictability of hand-hestureproperties. In Procceedings of the International Conference on Autonomous Agents and Multiagent Systems (AAMAS '22). IFAAMAS, 770-779. 
 
[35] https://doi.org/10.1109/ICCV.2019.00085
 Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori, Siddhartha S. Srinivasa,and Yaser Sheikh. 2019. Talking With Hands 16.2M: A large-scale dataset of syn-chronized body-finger motion and audio for conversational motion analysis and synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV '19). IEEE, 763-772. 
 
[36] https://doi.org/10.1145/1778765.1778861
 Sergey Levine, Philipp Krähenbühl, Sebastian Thrun, and Vladlen Koltun. 2010.Gesture controllers. ACM Trans. Graph. 29, 4, Article 124 (2010), 11 pages. 
 
[37] https://doi.org/10.1609/aaai.v33i01.33016706
 Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. 2019. Neuralspeech synthesis with transformer network. In Proceedings of the AAAI Conferenceon Artificial Intelligence (AAAI '19, Vol. 33). 6706-6713. 
 
[38] https://doi.org/10.1109/ICASSP39728.2021.9414660
 JinHong Lu, TianHang Liu, ShuZhuang Xu, and Hiroshi Shimodaira. 2021. Double-DCCCAE: Estimation of body gestures from speech waveform. In Proceedingsof the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP '21). IEEE, 900-904. 
 
[39] Shuhong Lu and Andrew Feng. 2022. The DeepMotion entry to the GENEAChallenge 2022. In Proceedings of the ACM International Conference on MultimodalInteraction (ICMI '22 ACM.

[40] https://doi.org/10.1177/002383099403700208
 David McNeill. 1992. Hand and Mind: What Gestures Reveal about Thought.University of Chicago Press. 
 
[41] https://doi.org/10.21437/Interspeech.2020-2382
 Gabriel Mittag and Sebastian Möller. 2020. Deep learning based assessmentof synthetic speech naturalness. In Proceedings of the Annual Conference of theInternational peech ommunication Association (Interspeech '20). ISCA, 1748-1752. 
 
[42] https://doi.org/10.21437/Interspeech.2010-413
 Sebastian Möller, Florian Hinterleitner, Tiago H. Falk, and Tim Polzehl. 2010.Comparison of approaches for instrumentally predicting the quality of text-to-speech systems. n Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech '10). ISCA, 1325-1328. 
 
[43] Khaled Saleh. 2022. Hybrid seq2seq architecture for 3D co-speech gesture genera-tion. In Proceedings of the ACM International Conference on Multimodal Interaction(ICMI 22 ACM.
[44] https: //doi.org/10.1007/s12369-013-0196-9
 Maha Salem, Friederike Eyssel, Katharina Rohlfing, Stefan Kopp, and FrankJoublin. 2013. To err is human(-like): Effects of robot gesture on perceivedanthropomorphism and ikability. Int. J. Soc. Robot. 5, 3 (2013), 313-323. 
 
[45] https://doi. org/10.1109/ROMAN.2011.6005285
 Maha Salem, Katharina Rohlfing, Stefan Kopp, and Frank Joublin. 2011. A friendlygesture: Investigating the effect of multimodal robot behavior in human-robotinteraction. n roccedings of the IEEE International Symposium on Robot andHuman Interactive Communication (RO-MAN '11). IEEE, 247-252. 
 
[46] https://doi.org/10.1155/ 2009/191940
 Giampiero Salvi, Jonas Beskow, Samer Al Moubayed, and Björn Granström. 2009.SynFace—Speech-driven facial animation for virtual speech-reading support.EURASIP J. Audio pee., Article 191940 (2009), 10 pages. 
 
[47] https://doi.rg/10.1016/j.specom.2016.04.001
 Ibon Saratxaga, Jon Sanchez, Zhizheng Wu, Inma Hernaez, and Eva Navas. 2016.Synthetic speech detection using phase information. Speech Commun. 81 (2016),30-41. 
 
[48] https://doi.org/10.1109/ICASSP.2018.8461368
 Jonathan Shen, Ruoming Pang, Ron J. Weiss, Mike Schuster, Navdeep Jaitly,Zongheng Yang, Zhifeng Chen, Yu Zhang, Yuxuan Wang, RJ Skerry-Ryan, Rif A.Saurous, Yannis giomyrgiannakis, and Yonghui Wu. 2018. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In Proceedings of theIEEE international conference on Acoustics, Speech and Signal Processing (ICASSP'18). IEEE, 4799-4783. 
 
[49] https://aclanthology.org/L12-1513/
 Éva Székely, João P. Cabral, Mohamed Abou-Zleikha, Peter Cahill, and JulieCarson-Berndsen. 2012. Evaluating expressive speech synthesis from audio-books in conversational hrases. In Proceedings of the International Conference on Language Resources and Evaluation (LREC '12). ELRA, 3335-3339. 
 
[50] https: //doi.org/10.1109/ECTI-CON51831.2021.9454931
 Ausdang Thangthai, Kwanchiva Thangthai, Arnon Namsanit, Sumonmas That-phithakkul, and Sittipong Saychum. 2021. Speech gesture generation from acoustic and textual information using LSTMs. In Proceedings of the Interna-tional Conference on Electrical Engineering/Electronics, Computer, Telecommu- nications and Information Technology (ECTI-CON '21). IEEE, 718-723. 
 
[51] https: //tech.ebu.ch/docs/r/r128.pdf
 European Broadcasting Union. 2020. Loudness normalisation and permittedmaximum level of audio signals. EBU Recommendation EBU R 128v4. 
 
[52] https://doi.org/ 10.1016/j.specom.2013.09.008
 Petra Wagner, Zofia Malisz, and Stefan Kopp. 2014. Gesture and speech ininteraction: An overview. Speech Commun. 57 (2014), 209-232. 
 
[53] https://doi.org/10.1145/3462244.3479914
 Siyang Wang, Simon Alexanderson, Joakim Gustafson, Jonas Beskow, Gustav EjeHenter, and Éva Székely. 2021. Integrated speech and gesture synthesis. In Proceedings of the ACM International Conference on Multimodal Interaction (ICMI'21). ACM, 177-185. 
 
[54] Jonathan Windle, David Greenwood, and Sarah Taylor. 2022. UEA Digital Humansentry to the GENEA Challenge 2022. In Proceedings of the ACM International Conference on Multimodal Interaction (ICMI '22 ACM.

[55] https://doi.org/10.1145/3462244.3479889
 Pieter Wolfert, Jeffrey M. Girard, Taras Kucherenko, and Tony Belpaeme. 2021.To rate or not to rate: Investigating evaluation methods for generated co-speech gestures. In Proceedings of the ACM International Conference on Multimodal Inter-action (ICMI '21). ACM, 494-502. 
 
[56] https://doi.org/10.1109/THMS.2022.3149173
 Pieter Wolfert, Nicole Robinson, and Tony Belpaeme. 2022. A review ofevaluation practices of gesture generation in embodied conversational agents. IEEE Transactions on Human-Machine Systems 52, 3 (2022), 379-389. 
 
[57] Sicheng Yang, Zhiyong Wu, Minglei Li, Mengchen Zhao, Jiuxin Lin, Liyang Chen,and Weihong Bao. 2022. The ReprGesture entry to the GENEA Challenge 2022. In Proceedings of the ACM International Conference on Multimodal Interaction(ICMI '22 ACM.

[58] https://openreview.net/forum?id=0Kj5mhn6sw
 Payam Jome Yazdian, Mo Chen, and Angelica Lim. 2021. Gesture2Vec: Clusteringgestures using representation learning methods for co-speech gesture generation. 
 
[59] https://doi.org/10.1145/3414685.3417838
 Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim,and Geehyuk Lee. 2020. Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Trans. Graph. 39, 6, Article 222 (2020),16 pages. 
 
[60] https://doi.org/10.1109/ICRA.2019.8793720
 Youngwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and GeehyukLee. 2019. Robots learn social skills: End-to-end learning of co-speech gesturegeneration for humanoid robots. In Proc. ICRA (ICRA '19). IEEE, 4303-4309.
 
[61] https://doi.org/10.1145/3472749.3474789
 Youngwoo Yoon, Keunwoo Park, Minsu Jang, Jaehong Kim, and Geehyuk Lee.2021. SGToolkit: An interactive gesture authoring toolkit for embodied conver-sational agents. In Proccedings of the Annual ACM Symposium on User InterfaceSoftware and Technology (UIST '21). ACM, 826-840. 
 
[62] https://doi.org/10.21437/Interspeech.2016-847
 Takenori Yoshimura, Gustav Eje Henter, Oliver Watts, Mirjam Wester, JunichiYamagishi, and Keiichi Tokuda. 2016. A hierarchical predictor of synthetic speechnaturalness using neural networks. In Proceedings of the Annual Conference of theInternational Speech Communication Association (Interspeech '16). ISCA, 342-346.
 
[63] Chi Zhou, Tengyue Bian, and Kang Chen. 2022. GestureMaster: Graph-basedspeech-driven gesture generation. In Proceedings of the ACM International Con-ference on Multimodal Interaction (ICMI '22 ACM.