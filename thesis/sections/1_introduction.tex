\section{Introduction}
\label{sec:introduction}
Gesturing is an important part of speaking. It adds emphasis and clarity to a speech and conveys essential non-verbal information that makes the speech lively and persuasive \cite{burgoon1990nonverbal}. There are rich demands for high-quality 3D gesture animation in many industries, such as games, films, and digital humans. However, the difficulties in reproducing the complex yet subtle harmony between vocalization and body movement make synthesizing natural-looking co-speech gestures remain a long-standing and challenging task.

Gestures are grouped into six categories by linguists \cite{ekman1969repertoire, mcneill1992hand}---adaptors, emblems, deictics, iconics, metaphorics, and beats. Among them, the beat gestures are rhythmic movements that bear no apparent relation to speech semantics \cite{Kipp2004_Gesture} but serve meta-narrative functions~\cite{mcneill1992hand} that are crucial to rhythmic harmony between speech and gestures. Generating realistic beat gestures requires modelling the relation between the gestural beats and the verbal stresses. However, it has been observed that these two modalities are not synchronized in a strict rhythmic sense \cite{mcclave1994gestural}, making it difficult to learn their temporal connection directly from data using an end-to-end method \cite{kucherenko2020gesticulator, yoon2020speech, bhattacharya2021speech2affectivegestures}. 

Gestures are associated with different levels of speech information~\cite{mcneill1992hand}. For example, an emblem gesture such as \emph{thumbs-up} usually accompanies high-level semantics like \emph{good} or \emph{great}, while a beat gesture commonly comes with low-level acoustic emphasis. Many previous studies use only the features extracted at the last layer of an audio encoder to synthesize gestures \cite{kucherenko2020gesticulator, yoon2020speech, bhattacharya2021speech2affectivegestures, qian2021speech, alexanderson2020style}. This setup, however, may in effect encourage the encoder to mix the speech information from multiple levels into the same feature, causing ambiguity and increasing the difficulty in mining clear rhythmic and semantic cues.

In this paper, we focus on generating co-speech upper-body gestures that can accompany a broad range of speech content---from a single sentence to a public speech, aiming at achieving convincing results both on the rhythm and semantics. Our first observation is that gesturing can be considered as a special form of dancing under changing beats. We develop a rhythm-based canonicalization and generation framework to deal with the challenge of generating synchronized gestures to the speech, which segments the speech into short clips at audio beats, normalizes these clips into canonical blocks of the same length, generates gestures for every block, and aligns the generated motion to the rhythm of the speech. This framework, which is partially inspired by recent research in dance generation~\cite{aristidou2021rhythm}, provides the gesture model with an explicit hint of the rhythm, allowing the model to learn the pattern of gestural beats within a rhythmic block efficiently. Both the quantitative evaluation with a novel rhythmic metric and the qualitative evaluation with user studies show that the gestures generated by this pipeline exhibit natural synchronization to the speech.

As indicated in linguistics literature~\cite{Kipp2004_Gesture,Neff2008Gesture,Webb1996_Linguistic}, gestures used in everyday conversation can be broken down into a limited number of semantic units with different motion variations. We assume that these semantic units, usually referred to as \emph{lexeme}s, relate to the high-level features of speech audio, while the motion variations are determined by the low-level audio features. We thus disentangle high- and low-level features from different layers of an audio encoder and learn the mappings between them and the gesture lexemes and the motion variations, respectively. Experiments demonstrate that this mechanism successfully disentangles multi-level features of both the speech and motion and synthesizes semantics-matching and stylized gestures.

In summary, our main contributions in this paper are:
\begin{itemize}
    \item We present a novel rhythm- and semantics-aware co-speech gesture synthesis system that generates natural-looking gestures. To the best of our knowledge, this is the first neural system that explicitly models both the rhythmic and semantic relations between speech and gestures.
    \item We develop a robust rhythm-based segmentation pipeline to ensure the temporal coherence between speech and gestures, which we find is crucial to achieving rhythmic gestures.
    \item We devise an effective mechanism to relate the disentangled multi-level features of both speech and motion, which enables generating gestures with convincing semantics.
\end{itemize}