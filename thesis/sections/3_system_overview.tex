\section{System Overview}
\label{sec:system_overview}
Our goal is to synthesize realistic co-speech upper-body gestures that match a given speech context both temporally and semantically. To achieve this goal, we build a system using neural networks that takes speech audio as input and generates gesture sequences accordingly. Additional speech modalities, such as text and speaker identity, will also be considered by the system when available to enhance semantic coherence and generate stylized gestures.

A gesture motion consists of a sequence of gesture units, which can be further broken down into a number of gesture phases that align with intonational units, such as pitch accents or stressed syllables \cite{kendon2004GestureBook,Loehr2012Temporal}. The action in each of these gesture phases is typically a specific movement such as lifting a hand, holding an arm at a position, or moving both arms down together, which is often referred to as a \emph{gesture lexeme} by linguists~\cite{Neff2008Gesture,Kipp2004_Gesture,Webb1996_Linguistic}. It is also revealed in the literature that there are only a limited number of lexemes used in everyday conversation. These lexemes form a \emph{gesture lexicon}. A typical speaker may only use a subset of this lexicon and apply slight variations to the motion. We assume such variations cannot be inferred directly from the speech but can be characterized by some latent variables, which we refer to as the \emph{gesture style code}s. Our system then generates gestures in a hierarchical order. It first determines the sequence of gesture lexemes and style codes and then generates gestural moves based on these motion-related features and other speech modalities.

Our system processes the input speech in a block-wise manner. Considering the temporal and structural synchrony between the gesture and the speech, we leverage a segmentation that aligns with the rhythm of the speech to ensure temporal coherence between the two modalities. Specifically, our system extracts beats from the input speech based on audio onsets and segments the speech into short clips at every beat. These clips are then time-scaled and converted into normalized blocks with the same length. We extract features at multiple levels for each block, where the high-level features are translated into a gesture lexeme, and the low-level features determine the style code. The generated gesture motions are then denormalized to match the length of the input speech.

As illustrated in \fig\ref{fig:system_overview}, our system consists of three core components: (a) the \emph{data} module preprocesses a speech, segments it into normalized blocks based on the beats, and extracts speech features from these blocks; (b) the \emph{training} module learns a gesture lexicon from the normalized motion blocks and trains the generator to synthesize gesture sequences, conditioned on the gesture lexemes, the style codes, as well as the features of previous motion blocks and adjacent speech blocks; and (c) the \emph{inference} module employs interpreters to transfer the speech features to gesture lexemes and style codes, which are then used by the learned generator to predict future gestures.

We train our system on a speech-gesture dataset with accompanying text and speaker identity (ID). The gesture lexicon is constructed using unsupervised learning based on a vector quantized variational autoencoder (VQ-VAE)~\cite{oord2017neural}. The generator is trained as an autoregressive encoder-decoder network, where we use an LSTM-based decoder combined with a vector quantized encoder to generate gesture motions. We train two separate interpreters to translate speech features into gesture lexemes and style codes, respectively. These interpreters can work with only the audio features and can be retrained to accept other speech modalities. In the following sections, we will provide details about these components and how they are trained in our system.
