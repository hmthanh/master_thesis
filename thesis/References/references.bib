% ###########################################
% Survey
% ###########################################
@inproceedings{bergmann2011relation,
  title={The relation of speech and gestures: Temporal synchrony follows semantic synchrony},
  author={Bergmann, Kirsten and Aksu, Volkan and Kopp, Stefan},
  booktitle={Proceedings of the 2nd Workshop on Gesture and Speech in Interaction (GeSpIn 2011)},
  year={2011},
  keyword={}
}

% ###########################################
% Rule base
% ###########################################
@inproceedings{huang2012robot,
  title={Robot behavior toolkit: generating effective social behaviors for robots},
  author={Huang, Chien-Ming and Mutlu, Bilge},
  booktitle={Proceedings of the seventh annual ACM/IEEE international conference on Human-Robot Interaction},
  pages={25--32},
  year={2012},
    keyword={}
}

% ###########################################
% Statistic
% ###########################################
@incollection{levine2010gesture,
  title={Gesture controllers},
  author={Levine, Sergey and Kr{\"a}henb{\"u}hl, Philipp and Thrun, Sebastian and Koltun, Vladlen},
  booktitle={Acm siggraph 2010 papers},
  pages={1--11},
  year={2010},
}


% ###########################################
% Deep learning
% ##########################################

@article{neff2008gesture,
  title={Gesture modeling and animation based on a probabilistic re-creation of speaker style},
  author={Neff, Michael and Kipp, Michael and Albrecht, Irene and Seidel, Hans-Peter},
  journal={ACM Transactions On Graphics (TOG)},
  volume={27},
  number={1},
  pages={1--24},
  year={2008},
  publisher={ACM New York, NY, USA},
keyword={}
}

@inproceedings{chiu2015predicting,
  title={Predicting co-verbal gestures: A deep and temporal modeling approach},
  author={Chiu, Chung-Cheng and Morency, Louis-Philippe and Marsella, Stacy},
  booktitle={Intelligent Virtual Agents: 15th International Conference, IVA 2015, Delft, The Netherlands, August 26-28, 2015, Proceedings 15},
  pages={152--166},
  year={2015},
  organization={Springer},
keyword={}
}

@inproceedings{kucherenko2020gesticulator,
  title={Gesticulator: A framework for semantically-aware speech-driven gesture generation},
  author={Kucherenko, Taras and Jonell, Patrik and Van Waveren, Sanne and Henter, Gustav Eje and Alexandersson, Simon and Leite, Iolanda and Kjellstr{\"o}m, Hedvig},
  booktitle={Proceedings of the 2020 international conference on multimodal interaction},
  pages={242--250},
  year={2020},
  keyword={}
}


@inproceedings{bhattacharya2021speech2affectivegestures,
  title={Speech2affectivegestures: Synthesizing co-speech gestures with generative adversarial affective expression learning},
  author={Bhattacharya, Uttaran and Childs, Elizabeth and Rewkowski, Nicholas and Manocha, Dinesh},
  booktitle={Proceedings of the 29th ACM International Conference on Multimedia},
  pages={2027--2036},
  year={2021},
  keyword={}
}

@inproceedings{hasegawa2018evaluation,
  title={Evaluation of speech-to-gesture generation using bi-directional LSTM network},
  author={Hasegawa, Dai and Kaneko, Naoshi and Shirakawa, Shinichi and Sakuta, Hiroshi and Sumi, Kazuhiko},
  booktitle={Proceedings of the 18th International Conference on Intelligent Virtual Agents},
  pages={79--86},
  year={2018}
}


@inproceedings{bhattacharya2021text2gestures,
  title={Text2gestures: A transformer-based network for generating emotive body gestures for virtual agents},
  author={Bhattacharya, Uttaran and Rewkowski, Nicholas and Banerjee, Abhishek and Guhan, Pooja and Bera, Aniket and Manocha, Dinesh},
  booktitle={2021 IEEE virtual reality and 3D user interfaces (VR)},
  pages={1--10},
  year={2021},
  organization={IEEE}
}


@inproceedings{wu2021probabilistic,
  title={Probabilistic human-like gesture synthesis from speech using GRU-based WGAN},
  author={Wu, Bowen and Liu, Chaoran and Ishi, Carlos T and Ishiguro, Hiroshi},
  booktitle={Companion Publication of the 2021 International Conference on Multimodal Interaction},
  pages={194--201},
  year={2021}
}

@article{xu2022freeform,
  title={Freeform body motion generation from speech},
  author={Xu, Jing and Zhang, Wei and Bai, Yalong and Sun, Qibin and Mei, Tao},
  journal={arXiv preprint arXiv:2203.02291},
  year={2022}
}

@article{ekman1969repertoire,
  title={The repertoire of nonverbal behavior: Categories, origins, usage, and coding},
  author={Ekman, Paul and Friesen, Wallace V},
  journal={semiotica},
  volume={1},
  number={1},
  pages={49--98},
  year={1969},
  publisher={De Gruyter}
}

@book{sebeok2011advances,
  title={Advances in visual semiotics: The semiotic web 1992-93},
  author={Sebeok, Thomas A and Umiker-Sebeok, Jean},
  volume={118},
  year={2011},
  publisher={Walter de Gruyter}
}

@book{kipp2005gesture,
  title={Gesture generation by imitation: From human behavior to computer character animation},
  author={Kipp, Michael},
  year={2005},
  publisher={Universal-Publishers}
}

@book{webb1997linguistic,
  title={Linguistic features of metaphoric gestures},
  author={Webb, Rebecca A},
  year={1997},
  publisher={University of Rochester}
}

@article{mcclave1994gestural,
  title={Gestural beats: The rhythm hypothesis},
  author={McClave, Evelyn},
  journal={Journal of psycholinguistic research},
  volume={23},
  number={1},
  pages={45--66},
  year={1994},
  publisher={Springer}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}


@article{starke2022deepphase,
  title={Deepphase: Periodic autoencoders for learning motion phase manifolds},
  author={Starke, Sebastian and Mason, Ian and Komura, Taku},
  journal={ACM Transactions on Graphics (TOG)},
  volume={41},
  number={4},
  pages={1--13},
  year={2022},
  publisher={ACM New York, NY, USA}
}

@inproceedings{yang2023qpgesture,
  title={QPGesture: Quantization-Based and Phase-Guided Motion Matching for Natural Speech-Driven Gesture Generation},
  author={Yang, Sicheng and Wu, Zhiyong and Li, Minglei and Zhang, Zhensong and Hao, Lei and Bao, Weihong and Zhuang, Haolin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2321--2330},
  year={2023}
}

@article{baevski2019vq,
  title={vq-wav2vec: Self-supervised learning of discrete speech representations},
  author={Baevski, Alexei and Schneider, Steffen and Auli, Michael},
  journal={arXiv preprint arXiv:1910.05453},
  year={2019}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}


% ###########################################
% Fundemental paper
% ##########################################

@article{van2017neural,
  title={Neural discrete representation learning},
  author={Van Den Oord, Aaron and Vinyals, Oriol and others},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@incollection{metallo2015scanning,
  title={Scanning and printing a 3D portrait of President Barack Obama},
  author={Metallo, Adam and Rossi, Vincent and Blundell, Jonathan and Waibel, G{\"u}nter and Graham, Paul and Fyffe, Graham and Yu, Xueming and Debevec, Paul},
  booktitle={SIGGRAPH 2015: Studio},
  pages={1--1},
  year={2015}
}

% ###########################################
% Diffusion Paper
% ##########################################


% [Ahuja et al., 2020] Chaitanya Ahuja, Dong Won Lee, Yukiko I. Nakano, and Louis-Philippe Morency. Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach. In Computer Vision ECCV, volume 12363 of Lecture Notes in Computer Science, pages 248-265, 2020.
@inproceedings{ahuja2020style,
  title={Style transfer for co-speech gesture animation: A multi-speaker conditional-mixture approach},
  author={Ahuja, Chaitanya and Lee, Dong Won and Nakano, Yukiko I and Morency, Louis-Philippe},
  booktitle={Computer Vision ECCV},
  volume={12363},
  pages={248--265},
  year={2020},
  organization={Springer}
}

% [Alexanderson et al., 2020a] Simon Alexanderson, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. Style-controllable speech-driven gesture synthesis using normalising flows. Comput. Graph. Forum, 39(2):487496, 2020.
@article{alexanderson2020style,
  title={Style-controllable speech-driven gesture synthesis using normalising flows},
  author={Alexanderson, Simon and Henter, Gustav Eje and Kucherenko, Taras and Beskow, Jonas},
  journal={Computer Graphics Forum},
  volume={39},
  number={2},
  pages={487--496},
  year={2020},
  publisher={Wiley Online Library}
}

% [Alexanderson et al., 2021] Simon Alexanderson, Éva Székely, Gustav Eje Henter, Taras Kucherenko, and Jonas Beskow. Generating coherent spontaneous speech and gesture from text. CoRR, abs/2101.05684, 2021.
@article{alexanderson2021generating,
  title={Generating coherent spontaneous speech and gesture from text},
  author={Alexanderson, Simon and Sz{\'e}kely, {\'E}va and Henter, Gustav Eje and Kucherenko, Taras and Beskow, Jonas},
  journal={CoRR},
  volume={abs/2101.05684},
  year={2021}
}


@inproceedings{alexanderson2020generating,
  title={Generating coherent spontaneous speech and gesture from text},
  author={Alexanderson, Simon and Sz{\'e}kely, {\'E}va and Henter, Gustav Eje and Kucherenko, Taras and Beskow, Jonas},
  booktitle={Proceedings of the 20th ACM International Conference on Intelligent Virtual Agents},
  pages={1--3},
  year={2020}
}

% [Alexanderson et al., 2022] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. CoRR, abs/2211.09707, 2022.
@article{alexanderson2022listen,
  title={Listen, denoise, action! audio-driven motion synthesis with diffusion models},
  author={Alexanderson, Simon and Nagy, Rajmund and Beskow, Jonas and Henter, Gustav Eje},
  journal={CoRR},
  volume={abs/2211.09707},
  year={2022}
}


% [Ao et al., 2022] Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and Libin Liu. Rhythmic gesticulator: Rhythm-aware co-speech gesture synthesis with hierarchical neural embeddings. ACM Trans. Graph., 41(6):209:1209:19, 2022.
@article{ao2022rhythmic,
  title={Rhythmic gesticulator: Rhythm-aware co-speech gesture synthesis with hierarchical neural embeddings},
  author={Ao, Tenglong and Gao, Qingzhe and Lou, Yuke and Chen, Baoquan and Liu, Libin},
  journal={ACM Trans. Graph.},
  volume={41},
  number={6},
  pages={209:1--209:19},
  year={2022},
  publisher={ACM}
}

% [Beltagy et al., 2020] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020.
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={CoRR},
  volume={abs/2004.05150},
  year={2020}
}

[Chang et al., 2022a] Che-Jui Chang, Sen Zhang, and Mubbasir Kapadia. The ivi lab entry to the genea challenge 2022 - a tacotron2 based method for co-speech gesture generation with locality-constraint attention mechanism. In Proceedings of the 2022 International Conference on Multimodal Interaction, ICMI '22, page 784-789, 2022.
@inproceedings{chang2022ivi,
  title={The ivi lab entry to the genea challenge 2022 - a tacotron2 based method for co-speech gesture generation with locality-constraint attention mechanism},
  author={Chang, Che-Jui and Zhang, Sen and Kapadia, Mubbasir},
  booktitle={Proceedings of the 2022 International Conference on Multimodal Interaction},
  pages={784--789},
  year={2022}
}

% [Chang et al., 2022b] Ziyi Chang, Edmund JC Findlay, Haozheng Zhang, and Hubert PH Shum. Unifying human motion synthesis and style transfer with denoising diffusion probabilistic models. arXiv preprint arXiv:2212.08526, 2022.
@article{chang2022unifying,
  title={Unifying human motion synthesis and style transfer with denoising diffusion probabilistic models},
  author={Chang, Ziyi and Findlay, Edmund JC and Zhang, Haozheng and Shum, Hubert PH},
  journal={arXiv preprint arXiv:2212.08526},
  year={2022}
}

% [Chen et al., 2022] Sanyuan Chen, Chengyi Wang, Zhengyang Ch Xiangzhan Yu, and Furu Wei. Wavlm: Large-scale self-supervised pre-training for full stack speech processing. IEEE J. Sel. Top. Signal Process., 16(6):1505-1518, 2022.
@article{chen2022wavlm,
  title={Wavlm: Large-scale self-supervised pre-training for full stack speech processing},
  author={Chen, Sanyuan and Wang, Chengyi and Chen, Zhengyang and Wu, Yu and Liu, Shujie and Chen, Zhuo and Li, Jinyu and Kanda, Naoyuki and Yoshioka, Takuya and Xiao, Xiong and others},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  volume={16},
  number={6},
  pages={1505--1518},
  year={2022},
  publisher={IEEE}
}


% [Dabral et al., 2022] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: A framework for denoising-diffusion-based motion synthesis. CoRR, abs/2212.04495, 2022.
@inproceedings{dabral2023mofusion,
  title={Mofusion: A framework for denoising-diffusion-based motion synthesis},
  author={Dabral, Rishabh and Mughal, Muhammad Hamza and Golyanik, Vladislav and Theobalt, Christian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9760--9770},
  year={2023}
}

% [Ghorbani et al., 2022] Saeed Ghorbani, Ylva Ferstl, Daniel Holden, Nikolaus F Troje, and Marc-André Carbonneau. Zeroeggs: Zero-shot example-based gesture generation from speech. arXiv preprint arXiv:2209.07556, 2022.
@inproceedings{ghorbani2023zeroeggs,
  title={ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech},
  author={Ghorbani, Saeed and Ferstl, Ylva and Holden, Daniel and Troje, Nikolaus F and Carbonneau, Marc-Andr{\'e}},
  booktitle={Computer Graphics Forum},
  volume={42},
  number={1},
  pages={206--216},
  year={2023},
  organization={Wiley Online Library}
}

% [Ginosar et al., 2019] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles of conversational gesture. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 3497-3506, 2019.
@inproceedings{ginosar2019learning,
  title={Learning individual styles of conversational gesture},
  author={Ginosar, Shiry and Bar, Amir and Kohavi, Gefen and Chan, Caroline and Owens, Andrew and Malik, Jitendra},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3497--3506},
  year={2019}
}

% [Habibie et al., 2021] Ikhsanul Habibie, Weipeng Xu, Dushyant Mehta, Lingjie Liu, Hans-Peter Seidel, Gerard Pons-Moll, Mohamed Elgharib, and Christian Theobalt Learning speech-driven $3 \mathrm{~d}$ conversational gestures from video. In IVA '21: ACM International Conference on Intelligent Virtual Agents, pages 101-108, 2021.
@inproceedings{habibie2021learning,
  title={Learning speech-driven 3d conversational gestures from video},
  author={Habibie, Ikhsanul and Xu, Weipeng and Mehta, Dushyant and Liu, Lingjie and Seidel, Hans-Peter and Pons-Moll, Gerard and Elgharib, Mohamed and Theobalt, Christian},
  booktitle={Proceedings of the 21st ACM International Conference on Intelligent Virtual Agents},
  pages={101--108},
  year={2021}
}

% [Habibie et al., 2022] Ikhsanul Habibie, Mohamed Elgharib, Kripasindhu Sarkar, Ahsan Abdullah, Simbarashe Nyatsanga, Michael Neff, and Christian Theobalt. A motion matching-based framework for controllable gesture synthesis from speech. In SIGGRAPH '22: Special Interest Group on Computer Graphics and Interactive Techniques Conference, pages 46:1-46:9, 2022.
@inproceedings{habibie2022motion,
  title={A motion matching-based framework for controllable gesture synthesis from speech},
  author={Habibie, Ikhsanul and Elgharib, Mohamed and Sarkar, Kripasindhu and Abdullah, Ahsan and Nyatsanga, Simbarashe and Neff, Michael and Theobalt, Christian},
  booktitle={ACM SIGGRAPH 2022 Conference Proceedings},
  pages={1--9},
  year={2022}
}

% [Ho and Salimans, 2022] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. CoRR, abs/2207.12598, 2022.
@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}

% [Ho et al., 2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851, 2020.
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}

% [Huber, 1992] Peter J Huber. Robust estimation of a location parameter. In Breakthroughs in statistics, pages 492-518. 1992.
@incollection{huber1992robust,
  title={Robust estimation of a location parameter},
  author={Huber, Peter J},
  booktitle={Breakthroughs in statistics: Methodology and distribution},
  pages={492--518},
  year={1992},
  publisher={Springer}
}

% [Kim et al., 2022] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-form language-based motion synthesis \& editing. arXiv preprint arXiv:2209.00349, 2022.
@inproceedings{kim2023flame,
  title={Flame: Free-form language-based motion synthesis \& editing},
  author={Kim, Jihoon and Kim, Jiseob and Choi, Sungjoon},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={7},
  pages={8255--8263},
  year={2023}
}

% [Kitaev et al., 2020] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In 8th International Conference on Learning Representations, ICLR April 26-30, 2020.
@article{kitaev2020reformer,
  title={Reformer: The efficient transformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

% [Kucherenko et al., 2021] Taras Kucherenko, Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, and Gustav Eje Henter. A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA challenge 2020. In IUI '21: 26th International Conference on Intelligent User Interfaces, April 13-17, pages 11-21, 2021.
@inproceedings{kucherenko2021large,
  title={A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020},
  author={Kucherenko, Taras and Jonell, Patrik and Yoon, Youngwoo and Wolfert, Pieter and Henter, Gustav Eje},
  booktitle={26th international conference on intelligent user interfaces},
  pages={11--21},
  year={2021}
}








% [Li et al., 2021b] Jing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang, Zhenyu He, and Linchao Bao. Audio2gestures: Generating diverse gestures from speech audio with conditional variational autoencoders. In 2021
@inproceedings{li2021audio2gestures,
  title={Audio2gestures: Generating diverse gestures from speech audio with conditional variational autoencoders},
  author={Li, Jing and Kang, Di and Pei, Wenjie and Zhe, Xuefei and Zhang, Ying and He, Zhenyu and Bao, Linchao},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={11293--11302},
  year={2021}
}

% [Li et al., 2022] Siyao Li, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: $3 \mathrm{~d}$ dance generation by actor-critic GPT with choreographic memory. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR June 1824, pages 11040-11049, 2022.
@inproceedings{siyao2022bailando,
  title={Bailando: 3d dance generation by actor-critic gpt with choreographic memory},
  author={Siyao, Li and Yu, Weijiang and Gu, Tianpei and Lin, Chunze and Wang, Quan and Qian, Chen and Loy, Chen Change and Liu, Ziwei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11050--11059},
  year={2022}
}


% [Liang et al., 2022] Yuanzhi Liang, Qianyu Feng, Linchao Zhu, Li Hu, Pan Pan, and Yi Yang. SEEG: semantic energized co-speech gesture generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 10463-10472, 2022.
@inproceedings{liang2022seeg,
  title={Seeg: Semantic energized co-speech gesture generation},
  author={Liang, Yuanzhi and Feng, Qianyu and Zhu, Linchao and Hu, Li and Pan, Pan and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10473--10482},
  year={2022}
}

% [Lin et al., 2022] Zhenghao Lin, Yeyun Gong, Yelong Shen, Tong Wu, Zhihao Fan, Chen Lin, Weizhu Chen, and Nan Duan. GENIE: large scale pre-training for text generation with diffusion model. CoRR, abs/2212.11685, 2022.
@article{lin2022genie,
  title={Genie: Large scale pre-training for text generation with diffusion model},
  author={Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Chen, Weizhu and Duan, Nan},
  journal={arXiv preprint arXiv:2212.11685},
  year={2022}
}

% [Liu et al., 2022] Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou. Learning hierarchical crossmodal association for co-speech gesture generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 10452-10462, 2022.
@inproceedings{liu2022learning,
  title={Learning hierarchical cross-modal association for co-speech gesture generation},
  author={Liu, Xian and Wu, Qianyi and Zhou, Hang and Xu, Yinghao and Qian, Rui and Lin, Xinyi and Zhou, Xiaowei and Wu, Wayne and Dai, Bo and Zhou, Bolei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10462--10472},
  year={2022}
}

% [Luvizon et al., 2023] Diogo Luvizon, Marc Habermann, Vladislav Golyanik, Adam Kortylewski, and Christian Theobalt. Scene-aware 3d multi-human motion capture from a single camera. CoRR, abs/2301.05175, 2023.
@inproceedings{luvizon2023scene,
  title={Scene-Aware 3D Multi-Human Motion Capture from a Single Camera},
  author={Luvizon, Diogo C and Habermann, Marc and Golyanik, Vladislav and Kortylewski, Adam and Theobalt, Christian},
  booktitle={Computer Graphics Forum},
  volume={42},
  number={2},
  pages={371--383},
  year={2023},
  organization={Wiley Online Library}
}

% [McGill et al., 1978] Robert McGill, John W Tukey, and Wayne A Larsen. Variations of box plots. The american statistician, 32(1):12-16, 1978.
@article{mcgill1978variations,
  title={Variations of box plots},
  author={McGill, Robert and Tukey, John W and Larsen, Wayne A},
  journal={The american statistician},
  volume={32},
  number={1},
  pages={12--16},
  year={1978},
  publisher={Taylor \& Francis}
}

% [Mei and Patel, 2022] Kangfu Mei and Vishal M. Patel. VIDM: video implicit diffusion models. CoRR, abs/2212.00235, 2022.
@inproceedings{mei2023vidm,
  title={Vidm: Video implicit diffusion models},
  author={Mei, Kangfu and Patel, Vishal},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={8},
  pages={9117--9125},
  year={2023}
}


% [Nyatsanga et al., 2023] Simbarashe Nyatsanga, Taras Kucherenko, Chaitanya Ahuja, Gustav Eje Henter, and Michael Neff. A comprehensive review of datadriven co-speech gesture generation. arXiv preprint arXiv:2301.05339, 2023.
@inproceedings{nyatsanga2023comprehensive,
  title={A Comprehensive Review of Data-Driven Co-Speech Gesture Generation},
  author={Nyatsanga, Simbarashe and Kucherenko, Taras and Ahuja, Chaitanya and Henter, Gustav Eje and Neff, Michael},
  booktitle={Computer Graphics Forum},
  volume={42},
  number={2},
  pages={569--596},
  year={2023},
  organization={Wiley Online Library}
}

% [Rae and Razavi, 2020] Jack W. Rae and Ali Razavi. Do transformers need deep long-range memory? In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7524-7529, 2020.
@article{rae2020transformers,
  title={Do transformers need deep long-range memory},
  author={Rae, Jack W and Razavi, Ali},
  journal={arXiv preprint arXiv:2007.03356},
  year={2020}
}

% [Ramesh et al., 2022] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.
@article{ramesh2022hierarchical,
  title={Hierarchical text-conditional image generation with clip latents},
  author={Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
  journal={arXiv preprint arXiv:2204.06125},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

% [Ren et al., 2022] Zhiyuan Ren, Zhihong Pan, Xin Zhou, and Le Kang. Diffusion motion: Generate text-guided 3d human motion by diffusion model. arXiv preprint arXiv:2210.12315, 2022.
@inproceedings{ren2023diffusion,
  title={Diffusion motion: Generate text-guided 3d human motion by diffusion model},
  author={Ren, Zhiyuan and Pan, Zhihong and Zhou, Xin and Kang, Le},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

% [Roy et al., 2021] Aurko Roy. Efficient content-based sparse attention with routing transformers
@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press One Rogers Street Cambridge MA 02142-1209 USA journals-info}
}

% [Tevet et al., 2022] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion diffusion model. arXiv preprint arXiv:2209.14916, 2022.
@article{tevet2022human,
  title={Human motion diffusion model},
  author={Tevet, Guy and Raab, Sigal and Gordon, Brian and Shafir, Yonatan and Cohen-Or, Daniel and Bermano, Amit H},
  journal={arXiv preprint arXiv:2209.14916},
  year={2022}
}

% [Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

% [Xie et al., 2022] Pan Xie, Qipeng Zhang, Zexian Li, Hao Tang, Yao Du, and Xiaohui Hu. Vector quantized diffusion model with codeunet for text-to-sign pose sequences generation. arXiv preprint arXiv:2208.09141, 2022.
@article{xie2022vector,
  title={Vector quantized diffusion model with codeunet for text-to-sign pose sequences generation},
  author={Xie, Pan and Zhang, Qipeng and Li, Zexian and Tang, Hao and Du, Yao and Hu, Xiaohui},
  journal={arXiv preprint arXiv:2208.09141},
  year={2022}
}


% [Yang et al., 2022] Sicheng Yang, Zhiyong Wu, Minglei Li, Mengchen Zhao, Jiuxin Lin, Liyang Chen, and Weihong Bao. The reprgesture entry to the GENEA challenge 2022. In International Conference on Multimodal Interaction, ICMI, November 7-11, pages 758-763, 2022.
@inproceedings{yang2022reprgesture,
  title={The ReprGesture entry to the GENEA Challenge 2022},
  author={Yang, Sicheng and Wu, Zhiyong and Li, Minglei and Zhao, Mengchen and Lin, Jiuxin and Chen, Liyang and Bao, Weihong},
  booktitle={Proceedings of the 2022 International Conference on Multimodal Interaction},
  pages={758--763},
  year={2022}
}


% [Yi et al., 2022] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael J Black. Generating holistic 3d human motion from speech. arXiv preprint arXiv:2212.04420, 2022.
@inproceedings{yi2023generating,
  title={Generating holistic 3d human motion from speech},
  author={Yi, Hongwei and Liang, Hualin and Liu, Yifei and Cao, Qiong and Wen, Yandong and Bolkart, Timo and Tao, Dacheng and Black, Michael J},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={469--480},
  year={2023}
}

% [Yoon et al., 2019] Youngwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Robots learn social skills: End-to-end learning of cospeech gesture generation for humanoid robots. In International Conference on Robotics and Automation, ICRA, pages 4303-4309, 2019.
@inproceedings{yoon2019robots,
  title={Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots},
  author={Yoon, Youngwoo and Ko, Woo-Ri and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong and Lee, Geehyuk},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={4303--4309},
  year={2019},
  organization={IEEE}
}

% [Yoon et al., 2020] Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Transactions on Graphics (TOG), 39(6):1-16, 2020.
@article{yoon2020speech,
  title={Speech gesture generation from the trimodal context of text, audio, and speaker identity},
  author={Yoon, Youngwoo and Cha, Bok and Lee, Joo-Haeng and Jang, Minsu and Lee, Jaeyeon and Kim, Jaehong and Lee, Geehyuk},
  journal={ACM Transactions on Graphics (TOG)},
  volume={39},
  number={6},
  pages={1--16},
  year={2020},
  publisher={ACM New York, NY, USA}
}

% [Yoon et al., 2022] Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, Carla Viegas, Teodor Nikolov, Mihail Tsakov, and Gustav Eje Henter. The GENEA challenge 2022: A large evaluation of data-driven co-speech gesture generation. In International Conference on Multimodal Interaction, ICMI, November 7-11, pages 736-747, 2022.
@inproceedings{yoon2022genea,
  title={The GENEA Challenge 2022: A large evaluation of data-driven co-speech gesture generation},
  author={Yoon, Youngwoo and Wolfert, Pieter and Kucherenko, Taras and Viegas, Carla and Nikolov, Teodor and Tsakov, Mihail and Henter, Gustav Eje},
  booktitle={Proceedings of the 2022 International Conference on Multimodal Interaction},
  pages={736--747},
  year={2022}
}

% [Yuan et al., 2022] Ye Yuan, Jiaming Song, Umar Iqbal, Arash Vahdat, and Jan Kautz. Physdiff: Physicsguided human motion diffusion model. arXiv preprint arXiv:2212.02500, 2022.
@inproceedings{yuan2023physdiff,
  title={Physdiff: Physics-guided human motion diffusion model},
  author={Yuan, Ye and Song, Jiaming and Iqbal, Umar and Vahdat, Arash and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={16010--16021},
  year={2023}
}

% [Zhang et al., 2022] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022.
@article{zhang2022motiondiffuse,
  title={Motiondiffuse: Text-driven human motion generation with diffusion model},
  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2208.15001},
  year={2022}
}

% [Zhou and Wang, 2022] Zixiang Zhou and Baoyuan Wang. Ude: A unified driving engine for human motion generation. arXiv preprint arXiv:2211.16016, 2022.
@article{zhang2022motiondiffuse,
  title={Motiondiffuse: Text-driven human motion generation with diffusion model},
  author={Zhang, Mingyuan and Cai, Zhongang and Pan, Liang and Hong, Fangzhou and Guo, Xinying and Yang, Lei and Liu, Ziwei},
  journal={arXiv preprint arXiv:2208.15001},
  year={2022}
}

% [Zhou et al., 2022] Chi Zhou, Tengyue Bian, and Kang Chen. Gesturemaster: Graph-based speech-driven gesture generation. In International Conference on Multimodal Interaction, ICMI, pages 764-770, 2022.
@inproceedings{zhou2022gesturemaster,
  title={Gesturemaster: Graph-based speech-driven gesture generation},
  author={Zhou, Chi and Bian, Tengyue and Chen, Kang},
  booktitle={Proceedings of the 2022 International Conference on Multimodal Interaction},
  pages={764--770},
  year={2022}
}
